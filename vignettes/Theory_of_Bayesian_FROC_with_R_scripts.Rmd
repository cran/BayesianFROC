---
title: "Theory of Bayesian FROC with R scripts"
author: "Issei Tsunoda (  Please employ me !!  :'-D)  send me a mail: tsunoda.issei1111  _at_ gmail.com"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Theory for thresholds}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

 `r paste0("![\"X-ray of my teeth!!  Let's study togother with me !! :-D \"](",system.file("image", "a.jpg", package="BayesianFROC"),")")`



# Introduction

In this vignette, we provide Bayesian models for FROC analysis. Our work was inspired by a classical 1989 paper of Dev Chakraborty who treated it in the case of Maximal likelihood methods. A basic comparison of modality is discussed in terms of the posterior mean using hyperparameter representing AUC. We hope to take up some of these applications in Radiology.



# Contents of this vignettes



 - section 1: What is FROC __Data__ involving __hits__ and __false__ __alarms__?
 - section 2: Modeling
 - section 3: What is modality${}^{\dagger}$ __comparison__ ?
 - section 4: hierarchical model to __compare__ __modality__ in case of MRMC${}^{\dagger \dagger}$
 - section 5: Bayesian __ANOVA__ via _Bayes_ _factor_
 - section 6: Appendix


${}^{\dagger}$ _Modality_ is  treatment or imaging methods such as MRI, CT, PET, ... etc. 
 
 
${}^{\dagger \dagger}$ _MRMC_ is a traditional abbreviation for multiple reader and multiple _case_, where _case_ means modality

#  $\color{green}{\textit{Aim of FROC }}$

  - FROC analysis provides an evaluation of observer performance, by fitting FROC model, we get AUC representing how much doctor can detect lesions in radiogoraphs (images).

 - If images taken different methods, MRI, CT, PET,... (called _modality_), then there is a problem that
 
 $$\color{green}{\textit{which imaging method is best to detect signal such as lesion or nodule? }}$$
 
 For example, AUC of MRI is _higher_ than that of CT, then we may say that MRI is _better_ than CT to detect lesion. This is modality comparison issue. So, we want to obtain, e.g., the one of the following
 
 
 
$$\color{red}{\textit{MRI > PET > CT >.. }}$$
or

$$\color{red}{\textit{PET > CT > MRI >.. }}$$

or

$$\color{red}{\textit{CT > MRI > PET >.. }}$$
or ....


Another approach is the following null hypothesis $H_0$ that 

$$
 H_o: \color{red}{\textit{All modalities are same observer performance.}}
$$


##### Comparison of _modalilty_


 
  Radiograhs is associated with notion of _modality_, and  there are two case.

  
 - An example of modality is imaging method; MRI ,CT, PET, ... etc. Which imaging method is better to detect lesion? This is a modality comparison issue. 
 
 - If Radiographs of patients are grouped by  a treatment group (case) and the another group is another treatment (control), then in this context,  modality means treatment.
 



Anyway, the aim of FROC is to compare modality
 

# $\color{green}{\textit{What is FROC  Data-set?}}$

## What is FROC task from which data arise

Provides what the following table means and how to arise.

 Confidence Level    | No. of Hits | No. of False alarms
 :-----|----------|-------------
5 = definitely present |  $H_{5}$ |  $F_{5}$ 
4 = probably present   |  $H_{4}$ |  $F_{4}$ 
3 = questionable          |  $H_{3}$ |  $F_{3}$ 
2 = subtle    |  $H_{2}$ |  $F_{2}$ 
1 = very subtle       |  $H_{1}$ |  $F_{1}$ 



where $H_1,H_2,...,H_c,...H_5$ and  $F_1,F_2,...,F_c,...,F_5$ are non negative integers, representing $\text{H}$its and $\text{F}$alse alarms.

 -  hit  =  True Positive = TP
 -  false alarm   =  False Positive = FP
 
The next section, we give the definition of $H_c,F_c$.

After the next section, please keep in mind the notations $H_c,F_c$.


## $\color{green}{\textit{There are three actors}}$ for FROC trial:

##### $\color{red}{\text{Images, Reader, Gold-standard}}$  

 - $\color{red}{\text{Images}}$, which contains shadows indicating signals or noises. Also it is called *__case__*. Signal means nodule, lesion, diseased ... etc.
 - $\color{red}{\text{Reader}}$, who try to detect lesions from shadows in radiographs. Also it is called *__observer__*
    - reader identifies his suspicious locations of from  shadows in images with his confidence level.
    - If reader thinks there is _one_ _more_ lesions then reader can answer _several_ suspicious locations for each image, so, the answer is _not_ dichotomous, it differs from the ROC analysis.
 - $\color{red}{\text{Gold Standard}}$, identifying true lesion locations for all images. One can assign each reader's suspicious location to  *__TP__* (True positive) or *__FP__* (False Positive), whether the location of reader is _correct_ with some  tolerance or not.
 
 
 

 
 
 
 
 
 In this package, we assume that the each image specified true lesion location which  are evaluated base on the gold standard and reader does not know such locations.

 If user dataset is not the above format but the Jafroc xlsx format, then use `BayesianFROC::convertFromJafroc()`
 
## Radiographs and FROC task

 Radiologist try to detect lesions from radiographs.
 In FROC task, there are radiographs, readers, and researcher who knows the truth (_gold_-_standard_).
 
 1) __Reader__ marks his suspicious locations with  his confidence level.${}^{\dagger}$ 
      - In each image, _mark_ _locations_ which he thinks it will be lesion 
      - Also, write number indicating his _confidene_ _level_ as follows;      
         -  __5__ = __Definitely__ lesion
         -  __4__ = __Probably__ lesion
         -  __3__ = Oh,.. may be,.. lesion
         -  __2__ = __subtle__
         -  __1__ = __very__ subtle

 2) By _gold_ _standard_, researcher evaluate reader's answer, that is,
   if reader correctly marked his suspicious location for some true lesion location then it is called hit (true positive). If not, then researcher associate such mis-marked location  with a false alarm ( false positive)
   
 3) By doing over all radiographs, we obtain FROC data.
 
 

${}^{\dagger}$  If reader thinks __one__ __more__ lesion exist in a __single__ image, then he can mark __one__ __more__ locations for a image, and it differs from the _ROC_ analysis, which allows each reader only __dichotomous__ answer whether lesion exists.
 
 
## $\color{blue}{\textit{Flow of FROC task}}$


###### $\color{blue}{\textit{0. Gold standard}}$
 - Suppose that $\color{red}{\text{two red circles}}$ indicate  lesions, i.e. signal, which should be detected by reader. ( I am not doctor, it may be __not__ lesion, this is only __fictitious__ assumptions)
 
 - Of course, such red circle does not exists, when reader try to find lesions.
 
  `r paste0("![\"Red circles representing true lesions\"](",system.file("image", "aa.jpg", package="BayesianFROC"),")")`
  

###### $\color{blue}{\textit{1. Localization}}$

- Reader marks the following two; 
    -  __Localize__ his _suspicious_ lesions 
    -  __Rating__ his __confidence__ level from the following numbers;

         -  5 = Definitely lesion
         -  4 = Probably lesion
         -  3 = Oh,.. may be,.. lesion
         -  2 = subtle
         -  1 = very subtle

 - In the figure, each yellow triangle means that the reader thinks it is lesion with his confidence level denoted by the inner number.


    `r paste0("![\"Yellow triangle indicates suspicious locations of reader \"](",system.file("image", "aaa.jpg", package="BayesianFROC"),")")`
    
    
###### $\color{blue}{\textit{2. Evaluate hits }}$

- The yellow triangle indicates reader's suspicious location and the one of them is very near the red circle, thus we may conclude it as hit.



`r paste0("![\" The yellow triangle with confidence level 5 is a hit! \"](",system.file("image", "aaaa.jpg", package="BayesianFROC"),")")`
    
  
###### $\color{blue}{\textit{3. Count false alarms}}$

-  The other marks are false alarms, because they are far from the true lesion location being represented by the red circles.



    `r paste0("![\"Falsel Alarms \"](",system.file("image", "aaaaa.jpg", package="BayesianFROC"),")")`
    
###### $\color{blue}{\textit{4. Summarizing for } \textbf{ only one } \textit{image.}}$

Summarizing the above, 

 Confidence Level    | No. of Hits | No. of False alarms
 :-----|----------|-------------
5 = definitely present |  $H_{5}\color{red}{=1}$ |  $F_{5}\color{red}{=0}$ 
4 = probably present   |  $H_{4}\color{red}{=0}$ |  $F_{4}\color{red}{=0}$ 
3 = equivocal          |  $H_{3}\color{red}{=0}$ |  $F_{3}\color{red}{=1}$ 
2 = probably absent    |  $H_{2}\color{red}{=0}$ |  $F_{2}\color{red}{=0}$ 
1 = questionable       |  $H_{1}\color{red}{=0}$ |  $F_{1}\color{red}{=2}$ 


Note that this is done only for the _one_ image, and actual trial, there are _many_ _images_, so number of hits and false alarms are larger.



###### $\color{blue}{\textit{5. Evaluate for } \textbf{ all } \textit{  images.}}$

Summarizing for the __1-st__ image as an above example, 

 Confidence Level    | No. of Hits | No. of False alarms
 :-----|----------|-------------
5 = definitely present |  $H_{5}\color{red}{=1}$ |  $F_{5}\color{red}{=0}$ 
4 = probably present   |  $H_{4}\color{red}{=0}$ |  $F_{4}\color{red}{=0}$ 
3 = equivocal          |  $H_{3}\color{red}{=0}$ |  $F_{3}\color{red}{=1}$ 
2 = probably absent    |  $H_{2}\color{red}{=0}$ |  $F_{2}\color{red}{=0}$ 
1 = questionable       |  $H_{1}\color{red}{=0}$ |  $F_{1}\color{red}{=2}$ 



Summarizing for the __2-nd__ image, (which is not shown in the above)
 
  Confidence Level    | No. of Hits | No. of False alarms
 :-----|----------|-------------
5 = definitely present |  $H_{5}\color{red}{=0}$ |  $F_{5}\color{red}{=0}$ 
4 = probably present   |  $H_{4}\color{red}{=1}$ |  $F_{4}\color{red}{=1}$ 
3 = equivocal          |  $H_{3}\color{red}{=0}$ |  $F_{3}\color{red}{=0}$ 
2 = probably absent    |  $H_{2}\color{red}{=1}$ |  $F_{2}\color{red}{=1}$ 
1 = questionable       |  $H_{1}\color{red}{=0}$ |  $F_{1}\color{red}{=1}$ 

Summarizing for the __3-rd__ image, (which is not shown in the above)


 Confidence Level    | No. of Hits | No. of False alarms
 :-----|----------|-------------
5 = definitely present |  $H_{5}\color{red}{=0}$ |  $F_{5}\color{red}{=0}$ 
4 = probably present   |  $H_{4}\color{red}{=0}$ |  $F_{4}\color{red}{=3}$ 
3 = equivocal          |  $H_{3}\color{red}{=1}$ |  $F_{3}\color{red}{=1}$ 
2 = probably absent    |  $H_{2}\color{red}{=0}$ |  $F_{2}\color{red}{=3}$ 
1 = questionable       |  $H_{1}\color{red}{=0}$ |  $F_{1}\color{red}{=3}$ 

Summarizing for the __4-th__ image, 

Summarizing for the __5-th__ image, 

Summarizing for the __6-th__ image, 

Summarizing for ...., 

Summarizing for ...., 

Summarizing for ...., 

Summarizing for ...., 

.....................,

.....................,

.....................,


Summarizing for the $N_I$-th image, 


 - Consequently, summing  for $\color{red}{\textit{ all }}$ image results we ....




###### $\color{blue}{\textit{6. Obtain FROC data}}$

Summing (pooling) for _all_  images, we will get, e.g., the following FROC data


 Confidence Level    | No. of Hits | No. of False alarms
 :-----|----------|-------------
5 = definitely present |  $H_{5}\color{red}{=112}$ |  $F_{5}\color{red}{=21}$ 
4 = probably present   |  $H_{4}\color{red}{=56}$ |  $F_{4}\color{red}{=33}$ 
3 = equivocal          |  $H_{3}\color{red}{=44}$ |  $F_{3}\color{red}{=65}$ 
2 = probably absent    |  $H_{2}\color{red}{=32}$ |  $F_{2}\color{red}{=78}$ 
1 = questionable       |  $H_{1}\color{red}{=11}$ |  $F_{1}\color{red}{=122}$ 





Note that this is done only for the _one_ image, and actual trial, there are _many_ _images_, so number of hits and false alarms are larger.




## $\color{green}{\textit{Example Data from R Console }}$ 
 

In the above we use 5 confidence, but the following example is 3 confidence.

### Data  Single reader and single modality from R console

```{r}
#1) Build  data for singler reader and single modality  case.





  dataList <- list(
               c=c(3,2,1),     # c is ignored, can omit.
               h=c(97,32,31),
               f=c(1,14,74),
               NL=259,
               NI=57,
               C=3
               )





#  where,
#        c denotes confidence level, each components indicates that 
#                3 = Definitely lesion,
#                2 = subtle,  
#                1 = very subtle
#        h denotes number of hits 
#          (True Positives: TP) for each confidence level,
#        f denotes number of false alarms
#          (False Positives: FP) for each confidence level,
#        NL denotes number of lesions (signal),
#        NI denotes number of images,






BayesianFROC::viewdata(dataList)
```


#### To create  data, use the one of the followings:

 - `dataset_creator_new_version()`
 - `create_dataset()`
 -  `convertFromJafroc()`
 
The third function `convertFromJafroc()` converts the Jafroc formulation to the authors formulation.

# $\color{green}{\textit{Modeling}}$


 `r paste0("![\"My face! My health is bad, my prurigo nodularis bother me.\"](",system.file("image", "a.jpg", package="BayesianFROC"),")")`

 - single reader and single modality
 - multiple reader and multiple modality
 
 It would be difficult for someone, but do not bother you if you cannot understand, since it is all my responsibility

## Parameter

In Statistical __conventional__ notation, the parameter of model is denoted by $\theta$ and a likelihood by $f(y|\theta)$.

In FROC model, we may say that $\theta = (\mu,\sigma, z_1 ;dz_1,dz_2,...,dz_{C-1})$. 
In the following we show what these notations means.



 



## FROC data


 If number of confidence level is five ($C=5$), then the data-set for FROC analysis is the follows;
 
 
 Confidence Level    | No. of Hits | No. of False alarms
 :-----|----------|-------------
5 = definitely present |  $H_{5}$ |  $F_{5}$ 
4 = probably present   |  $H_{4}$ |  $F_{4}$ 
3 = equivocal          |  $H_{3}$ |  $F_{3}$ 
2 = probably absent    |  $H_{2}$ |  $F_{2}$ 
1 = questionable       |  $H_{1}$ |  $F_{1}$ 

In the above table, the $H_!,H_2,...,H_5$ and  $F_!,F_2,...,F_5$ are non-negative integers.
Confidence level is  sometimes called *__rating__* in other books.


## Modeling: __Single__ reader and __single__ modality


Statistical modeling is equivalent to calculating the probability of arising a data $(H_c,F_c),c=1,2,...,5$ for number of lesions $N_L$ and number of images $N_I$.

FROC data is very simple, that is, data are  hits $H_c$ and false alarms $F_c$.

First we focus on the _false_ _positive_ _fraction_ per image. In traditional statistics, it natural to assume that false alarms are distributed by the Poisson, that is,


$$ \frac{F_c+F_{c+1}+...+F_C}{N_I} \sim \text{Poisson}(\lambda_c)$$

When someone see the left hand side, he might consider what the hell?, does not know such a quantity,
but I am afraid that it is a famous quantity in ROC or FROC analysis and it is called False Positive Fraction (FPF) per image. So, please love it.

However, the false positive fraction, that is, the left hand side is _not_ a natural number, thus instead of the above, we assume the following;

$$F_c+F_{c+1}+...+F_C \sim \text{Poisson}(\lambda_cN_I)$$


or equivalently (see, Appendix), 


$$F_{c }   \sim  \text{Poisson} ( (\lambda _{c} -\lambda _{c+1} )\times N_{I} ),$$

Especially, 

$$F_{C }   \sim  \text{Poisson} ( (\lambda _{C} -0 )\times N_{I} ),$$




where $\lambda_c$ is a non negative number.  we may say $\lambda_c$ is a false alarm  rate  per image for generating the $c$-th FPF or False Positives per Image (FPI).


#### Hits

Next, we shall show the hit.

Each hit is associated with some lesion, so it natural to assume  the following;


$$H_{c }   \sim  \text{Binomial} ( p_{c}, N_{L} ),$$

where $p_c$ is a hit rate.


## Determine the rate of hits and false alarms

To determine the rate $p_c, \lambda_c$ we use the so-called *bi-normal* *assumptions* which may also be called a *__latent__* *Gaussian* *assumption.* The word *latent* indicates that  it  cannot be observed nor measured from FROC trial.

We may consider the latent Gaussian random variable is such as some biomarker. The roll of biomarker has two, one is the same as the confidence level and the other one is the latent Gaussian.



The author first consider that we use two distributions, one is associated with each lesions and the another one is with each images. But now, I think, it is wrong or redundant.


Anyway, assume that 


 $$
 Y \sim \text{Normal}(\mu,\sigma ^2) \\
  X \sim \text{Normal}(0,1) \\
 $$
and thresholds $z_1 < z_2 < ... < z_{c} < ... < z_C$ where $z_c \in \mathbb{R}$.

Then we consider this latent Gaussian variable determine the hit rate $p_c$.

$$p_{c}=\text{Prob}( z_c < Y <z_{c+1} )\\
=\Phi (\frac{z_{c +1}-\mu}{\sigma})-\Phi (\frac{z_{c}-\mu}{\sigma}). $$


In particular, we use the following definition

$$p_{C}= 1-\Phi (\frac{z_{c}-\mu}{\sigma}), $$

instead of 

$$p_{C}= \Phi (\frac{z_{C +1}-\mu}{\sigma}) -\Phi (\frac{z_{C}-\mu}{\sigma}), $$
with theoretically infinity but numerically very large number $z_{C+1}$. Do not confuse our model does not use the $z_{C+1}$ which cause the divergent transition issues, see Appendix for more detail.


Why does we define hit rate as above ? _Intuitively_, this binomial assumption says that each lesion (signal) assigned some latent variable $Y$ and if it _fall_ into the interval $z_c < Y <z_{c+1}$, then reader thinks it is his _suspicious_ positive. 






It shows that latent variable  merely decides the rate and not the hit.
Thus if we consider the latent Gaussian has i.i.d. and associated with lesion, then if value of it between $z_c$ and $z_{c+1}$, it cannot be said that it generates hit. But it may say that it would be hit in the probability $p_c$.


Using noise distribution, it natural to thinks that the probability of that the false positive is not zero with confidence level is greater than $c$ is 


$$\text{Prob}( z_c < X  )
=1- \Phi (z_c). $$



On the other hand, by the Poisson, it can be calculated (see Appendix)

$$\mathbb{P} \bigg\{ \frac{F_c+F_{c+1}+...+F_C}{N_I} \neq 0  \bigg\} = 1 - \mathbb{P} \bigg\{ \frac{F_c+F_{c+1}+...+F_C}{N_I} =0 \bigg\} \\
=1-e^{-\lambda_c}.$$




Now, we assume that 

$$\text{Prob}( z_c < X  )
=\mathbb{P} \bigg\{ \frac{F_c+F_{c+1}+...+F_C}{N_I} \neq 0  \bigg\} . $$

Then, we get 
$$
\lambda_c = - \log \Phi (z_c).
$$

Consequently, we get the Bayesian model;

$$H_{c }   \sim  \text{Binomial} ( p_{c}, N_{L} ),$$
$$F_{c }   \sim  \text{Poisson} ( (\lambda _{c} -\lambda _{c+1} )\times N_{I} ),$$
$$\lambda _{c} = - \log \Phi ( z_{c } ),$$
$$p_{c}
=\Phi (\frac{z_{c +1}-\mu}{\sigma})-\Phi (\frac{z_{c}-\mu}{\sigma}). $$
In this model, $z_{c},c=1,\cdots,C$, $\mu$, and $\sigma$ are the parameters to be estimated.


##### $\color{green}{\textit{Priors}}$
 
## Prior for  monotonicity  of thresholds

Thresholds $z_1, z_2,  ...., z_C$ should satisfy the monotonicity condition $z_1 < z_2 < .... < z_C$.

To do so, we use the prior that 
$$z_2 - z_1 \sim \text{Uniform}(0,\infty) \\ 
  z_3 - z_2 \sim \text{Uniform}(0,\infty) \\ 
  :\\
  :\\
  z_C - z_{C-1} \sim \text{Uniform}(0,\infty) \\ 
  $$
  where $\text{Uniform}(0,\infty)$ means improper prior whose support is the interval $(0,\infty)$.
  
We introduce a new parameter:
$$
dz_c:=z_{c+1}-z_{c},
$$
where $c=1,2,\cdots,C$. In practical modeling, we no longer use the previous section's parameters $z_2,z_3,\cdots, z_C$, using $ dz_1,dz_2,\cdots, dz_{C-1}$ instead. To include an order constraint for Bayesian models, we assume a non-regular uniform prior for $dz_c$:
$$
dz_c \sim \text{Uniform}(0, \infty),
$$
where $\text{Uniform}(0,\infty)$ indicates flat improper priors whose integrated values are not one and its support is the interval $(0,\infty)$. These priors are equivalent in that we assume monotonicity in the thresholds, as follows:
$$ z_{1}\leq z_{2}\leq z_{3} \leq \dots \leq z_{C}.$$







##### $\color{green}{\textit{Finally, we obtain the following Bayesian FROC model in case of single reader and single modality.
}}$





\begin{eqnarray*}
H_{c } & \sim &\text{Binomial} ( p_{c}, N_{L} ), \text{ for $c=1,2,...,C$.}\\
F_{c } & \sim &\text{Poisson}( (\lambda _{c} -\lambda _{c+1} )\times N_{L} ), \text{ for $c=1,2,...,C-1$.}\\
\lambda _{c}& =& - \log \Phi ( z_{c } ),\text{ for $c=1,2,...,C$.}\\
p_{c}
&=&\Phi (\frac{z_{c +1}-\mu}{\sigma})-\Phi (\frac{z_{c}-\mu}{\sigma}), \text{ for $c=1,2,...,C-1$.}\\
p_C & =& 1-\Phi (\frac{z_{C}-\mu}{\sigma}),\\
F_{C}  & \sim & \text{Poisson}( (\lambda _{C} - 0)N_I),\\
dz_c=z_{c+1}-z_{c} &\sim& \text{Uniform}(0,\infty), \text{ for $c=1,2,...,C-1$.}\\
 \mu &\sim& \text{Uniform}(-\infty,\infty),\\
 \sigma &\sim& \text{Uniform}(0,\infty),\\
\end{eqnarray*} 
Our model has parameters $z_{1}, dz_1,dz_2,\cdots, dz_{C-1}$, $\mu$, and $\sigma$. Notation $\text{Uniform}( -\infty,100000)$ means the improper uniform distribution of its support is the unbounded interval $( -\infty,100000)$.


The last three equation are priors. In past,



It might be redundant, but I think reader's back ground cannot be not inferred, so, I write down the following redundant descriptions.
If $C=5$, then it can be written without abbreviation as follows;




In the case of $C=5$, we exclude the subscript notation $c$ from the above model. I hope it helps reader. 
Provides the above model without abbreviation as follows;




\begin{eqnarray*}
H_{1 } & \sim &\text{Binomial} ( p_{1}, N_{L} ) \\
H_{2 } & \sim &\text{Binomial} ( p_{2}, N_{L} ) \\
H_{3 } & \sim &\text{Binomial} ( p_{3}, N_{L} ) \\
H_{4 } & \sim &\text{Binomial} ( p_{4}, N_{L} ) \\
H_{5 } & \sim &\text{Binomial} ( p_{4}, N_{L} ) \\
F_{1 } & \sim &\text{Poisson}( (\lambda _{1} -\lambda _{2} )\times N_{I} ), \\
F_{2 } & \sim &\text{Poisson}( (\lambda _{2} -\lambda _{3} )\times N_{I} ), \\
F_{3 } & \sim &\text{Poisson}( (\lambda _{3} -\lambda _{4} )\times N_{I} ), \\
F_{4 } & \sim &\text{Poisson}( (\lambda _{4} -\lambda _{5} )\times N_{I} ), \\
F_{5 } & \sim &\text{Poisson}( (\lambda _{5} -0  )\times N_{I} ), \text{Be careful !!:'-D}\\
\lambda _{1}& =& - \log \Phi ( z_{1 } ),\\
\lambda _{2}& =& - \log \Phi ( z_{2 } ),\\
\lambda _{3}& =& - \log \Phi ( z_{3 } ),\\
\lambda _{4}& =& - \log \Phi ( z_{4 } ),\\
\lambda _{5}& =& - \log \Phi ( z_{5 } ),\\
p_{1}
&:=&\Phi (\frac{z_{2}-\mu}{\sigma})-\Phi (\frac{z_{1}-\mu}{\sigma}),  \\
p_{2}
&:=&\Phi (\frac{z_{3}-\mu}{\sigma})-\Phi (\frac{z_{2}-\mu}{\sigma}),  \\
p_{3}
&:=&\Phi (\frac{z_{4}-\mu}{\sigma})-\Phi (\frac{z_{3}-\mu}{\sigma}),  \\
p_{4}
&:=&\Phi (\frac{z_{5}-\mu}{\sigma})-\Phi (\frac{z_{4}-\mu}{\sigma}),  \\
p_5 &:=& 1-\Phi (\frac{z_{5}-\mu}{\sigma}),\text{Be careful !!:'-D}\\
dz_1=z_{2}-z_{1} &\sim& \text{Uniform}(0,\infty),  \\
dz_2=z_{3}-z_{2} &\sim& \text{Uniform}(0,\infty),  \\
dz_3=z_{4}-z_{3} &\sim& \text{Uniform}(0,\infty),  \\
dz_4=z_{5}-z_{4} &\sim& \text{Uniform}(0,\infty),  \\
 \mu &\sim& \text{Uniform}(-\infty,\infty),\\
 \sigma &\sim& \text{Uniform}(0,\infty),\\
\end{eqnarray*} 
Our model has parameters $z_{1}, dz_1,dz_2,\cdots, dz_{C-1}$, $\mu$, and $\sigma$.  Notation $\text{Uniform}( -\infty,100000)$ means the improper uniform distribution of its support is the unbounded interval $( -\infty,100000)$.


#### R code to fit the above model to data

This is a basic example which shows how to fit a model to data `dataList` of single reader and single modality.


Note that in the following R scripts, a list object `dataList`  representing the following FROC data;


 Number of Confidence Level    | Number of Hits | Number of False alarms
 :-----|:----------:|:-------------:
3 = definitely present |  97 |  1 
2 = equivocal          |  32 |  14 
1 = questionable       |  31 |  74 


```{r, eval = FALSE}
#1) Build  data for singler reader and single modality  case.





  dataList <- list(
               c=c(3,2,1),     # c is ignored, can omit.
               h=c(97,32,31),
               f=c(1,14,74),
               NL=259,
               NI=57,
               C=3
               )




#  where,
#        c denotes confidence level, each components indicates that 
#                3 = Definitely lesion,
#                2 = subtle,  
#                1 = very subtle
#        h denotes number of hits 
#          (True Positives: TP) for each confidence level,
#        f denotes number of false alarms
#          (False Positives: FP) for each confidence level,
#        NL denotes number of lesions (signal),
#        NI denotes number of images,






#2) Fit the FROC model.


 
                  fit <- BayesianFROC::fit_Bayesian_FROC(dataList)




#3)  validation of fit via alculation of p -value of the chi square goodness of fit, which is 
#  calculated by integrating with  predictive posterior measure.
                  
                  
                   p_value_of_the_Bayesian_sense_for_chi_square_goodness_of_fit(fit)
                   
                   

                                     
                  
```




In the last R script `p_value_of_the_Bayesian_sense_for_chi_square_goodness_of_fit(fit)` we calculates p value of the chi square goodness of fit by Bayesian manner, so we shall briefly recall how it calculates the p value ( for more details, see Gelmann's book _Bayesian_ _Data_ _Analysis_).





In the following, we shall use the general notation.
Let $y_{\text{obs}}$ be observed data and $f(y|\theta)$ be a model (likelihood) for a future data $y$. We write a prior and a posterior distributions by $\pi(\theta)$ and $\pi(\theta|y) \propto f(y|\theta)\pi(\theta)$. The posterior predictive distribution  is defined by $p(y|y_{\text{obs}}) := \int f(y|\theta)\pi(\theta|y_{\text{obs}}) d\theta$


In our case, the data $y$ is a pair of hits and false alarms, that is $y=(H_1,H_2, \dots H_C; F_1,F_2, \dots F_C)$ and $\theta = (z_1,dz_1,dz_2,\dots,dz_{C-1},\mu, \sigma)$. We shall define the $\chi^2$ discrepancy ( goodness of fit statistics ) to validate that the model fit the data.

\[
T(y,\theta) := \sum _{c=1,\cdots,C} \biggr( \frac{\bigl\{H_c-N_L\times p_c \bigr\}^2}{N_L\times p_c}+
\frac{\bigl\{F_c-(\lambda _{c} -\lambda _{c+1} )\times N_{I}\bigr\}^2}{(\lambda _{c} -\lambda _{c+1} )\times N_{I} }\biggr).
\]



for single reader and single modality.

\[
T(y,\theta) := \sum _{r=1, \cdots,R} \sum _{m=1,\cdots,M} \sum _{c=1,\cdots,C} \biggr( \frac{\bigl\{H_{c,m,r}-N_L\times p_{c,m,r}\bigr\}^2}{N_L\times p_{c,m,r}}+
\frac{\bigl\{F_{c,m,r}-(\lambda _{c} -\lambda _{c+1} )\times N_{L}\bigr\}^2}{(\lambda _{c} -\lambda _{c+1} )\times N_{L} }\biggr).
\]

for multipler reader and multiple modality, where false positive fraction is per lesion istead of per image.
Of course it is very easy to implement per image hierarchical Bayesian model, so the author is kind of goofing  off in implementing per image model.




Note that $p_c$ and $\lambda _{c}$ depend on $\theta$.
In this statistic, the number of degrees of freedom is $C-2$.




Classical frequentist methods, the parameter $\theta$ is a fixed estimates, e.g. the maximal likelihood estimator, however, in Bayesian context, the parameter is not deterministic, so, by integrating with the posterior predictive distribution, we can get the posterior predictive $\chi^2$ value and its p-value. Let $y_{\text{obs}}$ be observed data. Then the posterior predictive p value is defined by

\[
p \text{ value of $y_{\text{obs}}$ in Theory}  = \int \int dy d\theta I_{T(y,\theta) > T(y_{\text{obs}},\theta)}f(y|\theta)\pi(\theta|y_{\text{obs}})
\]



In the following, we show how to calculate the double Integral. Suppose that  $\theta _1, \theta_2,\cdots,\theta_N$ are samples from the posterior distribution via Hamiltonian Monte Carlo simulation, we obtain a sequence of models (likelihoods) ;  $f(y|\theta_1),f(y|\theta_2),\cdots, f(y|\theta_N)$. 
 Drawing the samples $y_1,y_2,\cdots, y_N$ so that each $y_i$ is a sample from the distribution whose density function is $f(y|\theta_i)$, 
and it is desired one, that is, we can interpret that the samples $y_1,y_2,\cdots, y_N$ is drawing from the posterior predictive distributions.  Using the law of large number, we can calculate the noble integral of the p value by 



\[
p \text{ value approximation to calculate numerically}  =\frac{1}{N} \sum I_{T(y_i,\theta_i) > T(y_{\text{obs}},\theta_i)}
\]


 













 ...oh ache,.. ache ache my body is now very bad ,,, I hate prurigo nodularis. 


#####

I exposed some surfactants made by Flower King (fictitious name), then my body become bad, and many many very many prurigo noudularis appeared my whole body, and I struggled it for 1 years and 17 months. I want to say to live in safe, the bio sarfanct is very important, OK ? Bio !! not petroleum. Please make and develop  bio sarfactant for the future life.



# $\color{green}{\textit{Multiple reader and Multiple case }}$

 



 `r paste0("![\"X-ray of my teeth!!\"](",system.file("image", "a.jpg", package="BayesianFROC"),")")`
 
 
 
 
Note that Multiple reader and multiple $\color{red}{\text{"case"}}$ (MRMC) implies that multiple reader and multiple $\color{red}{\text{"modality"}}$ !!

So, I hate the word MRMC, it should be $\color{red}{\text{"MRMM"}}$ !!



## Data for MRMC
### Example.


Two readers and two modalities and three kind of confidence levels.

  Confidence Level| Modality ID| Reader ID    | Number of Hits | Number of False alarms
---|----| :-----|----------|-------------
3 = definitely present |1|1|  $H_{3,1,1}$ |  $F_{3,1,1}$ 
2 = equivocal          |1|1|  $H_{2,1,1}$ |  $F_{2,1,1}$ 
1 = questionable       |1|1|  $H_{1,1,1}$ |  $F_{1,1,1}$ 
3 = definitely present |1|2|  $H_{3,1,2}$ |  $F_{3,1,2}$ 
2 = equivocal          |1|2|  $H_{2,1,2}$ |  $F_{2,1,2}$ 
1 = questionable       |1|2|  $H_{1,1,2}$ |  $F_{1,1,2}$ 
3 = definitely present |2|1|  $H_{3,2,1}$ |  $F_{3,2,1}$ 
2 = equivocal          |2|1|  $H_{2,2,1}$ |  $F_{2,2,1}$ 
1 = questionable       |2|1|  $H_{1,2,1}$ |  $F_{1,2,1}$ 
3 = definitely present |2|2|  $H_{3,2,2}$ |  $F_{3,2,2}$ 
2 = equivocal          |2|2|  $H_{2,2,2}$ |  $F_{2,2,2}$ 
1 = questionable       |2|2|  $H_{1,2,2}$ |  $F_{1,2,2}$ 


where, each component $H$  and  $F$ are non negative integers. 

This package has example data, for example, the following object in this package is an MRMC dataset:
```{r,eval=FALSE}
BayesianFROC::dataList.Chakra.Web
```


##### To create such data, use the one of the followings:

 - `dataset_creator_new_version()`
 - `create_dataset()`
 -  `convertFromJafroc()`
 
The third function `convertFromJafroc()` converts the Jafroc formulation to the authors formulation.


#### Data of Multiple reader and multiple modality from R console

```{r}
BayesianFROC::viewdata(BayesianFROC::dd,head.only = TRUE)
```



Sorry to change the subject but I have pains in my arms, brain, my chest, got a itchy in my whole body, especially face.

So, go back to the statistical modeling, ah, .. OK. Next, we shall show the hierarchical Bayesian Model to compare modalities, i.e., compare imaging methods such as MRI, CT, PET, ... etc, or the another context modality would means the efficacy of treatments. Anyway, in the following we will show how to model.



Mathematical philosophy said that the expression of equation is not important, but the property or relation between notions is more important. So, in the following, I explain the hierarchical model which is very complex and the complex representation will hide the essence of the modeling. To avoid it, the author briefly show the essence or idea of the hierarchical modeling.


##### Essence of modeling of the hierarchical Bayesian Model.

In this brief section we use the notation $f(y|\theta)$ for a likelihood function. That is data is $y$ and model parameter is denoted by $\theta$.

The data format of MRMC case, the hits and false alarms are calculated for each reader and modality, thus the data has the form $y_{m,r}$ representing the hits and false alarms for the $m$-th modality and the $r$-th reader. So, we estimates the model parameter  $\theta_{m,r}$ representing AUCs for the $m$-th modality and the $r$-th reader.
To obtained the AUCs $\theta_{m}$ depend on modality only, we use the hyper parameter, such as

$$ \theta_{m,r} \sim \text{Normal}( \theta_m, \text{variance})$$.


So, parameter $\theta_m$ depends only on  the $m$-th modality. So, using the posterior of $\theta_m$, we obtains the following basic characteristics:

 - $\mathbb{P}(\theta_m > \theta_{m'} )$
 - $\theta_m  - \theta_{m'}$



#### Hierarchical model

The author of this model thinks it is better to show it without any explanation, since it is obvious;



\begin{eqnarray*}
H_{c,m,r} & \sim &\text{Binomial }( p_{c,m,r}, N_L ),\\
F_{c,m,r} &\sim& \text{Poisson }( ( \lambda _{c} - \lambda _{c+1})N_L ),\\
\lambda _{c}& =& - \log \Phi (z_{c }),\\
p_{c,m,r}
&:=&\Phi (\frac{z_{c +1}-\mu_{m,r}}{\sigma_{m,r}})-\Phi (\frac{z_{c}-\mu_{m,r}}{\sigma_{m,r}}), \\
\color{red}{p_C} & =& 1-\Phi (\frac{z_{C}-\mu_{m,r}}{\sigma_{m,r}}),\\
\color{red}{F_{C,m,r}}  & \sim &\text{Poisson } ( (\lambda _{C} - 0)N_I),\\
A_{m,r}&:=&\Phi (\frac{\mu_{m,r}/\sigma_{m,r}}{\sqrt{(1/\sigma_{m,r})^2+1}}), \\
A_{m,r}&\sim&\text{Normal} (A_{m},\sigma_{r}^2), \\
dz_c&:=&z_{c+1}-z_{c},\\
dz_c, \sigma_{m,r} &\sim& \text{Uniform}(0,\infty),\\
z_{c} &\sim& \text{Uniform}( -\infty,100000),\\
A_{m} &\sim& \text{Uniform}(0,1).\\
\end{eqnarray*} 
Our new model has parameters $z_{1}, dz_1,dz_2,\cdots, dz_{C}$, $A_{m}$, $\sigma_{r}$, $\mu_{m,r}$, and $\sigma_{m,r}$.








#### R code to fit the above model to data




 


## Work Flow  for MRMC case
 - Prepare __data__
    - Create by your hands: `dataset_creator_new_version()` or `create_dataset()`
    - Convert from Excel data of  Jafroc or  Rjafroc 
 format: `convertFromJafroc()`
    
- __Fitting__: `fit_Bayesian_FROC()`
    - Estimates of your FROC model
    - $\color{red}{\textit{ Comparison of Modalities }}$ (pairwise) 

- Draw __Curves__: `DrawCurves()`
    - FROC curve
    - AFROC curve
    - Cumulative False positive and cumulative true positives (FPF,TPF)
- *__Test__* the null hypothesis the all modalities are same via Bayes factor
    - Fit a null model, creating a `stanfit` object `fitHo` 
    - Fut a alternative model, creating a `stanfit` object `fitH1` 
    - evaluate the Bayes factor with these two stanfit objects `fitHo` and `fitH1`. 
    
    
    




## Work Flow with R script


Prepare data
```{r,eval=FALSE}
#An example dataset for the case of Multiple readers and Multiple Modalities.
dat  <- BayesianFROC::dataList.Chakra.Web
```

Fitting
```{r,eval=FALSE}
# Fitting  the hierarchical Bayesian model.
fit  <-  BayesianFROC::fit_Bayesian_FROC(dat)

```

Draw Curves
```{r,eval=FALSE}
 #Draw curves for the 1st modality and 2nd reader
 DrawCurves(

   # fitted model object 
        fit,
   
   # Modatity ID whose curves are drawn.
      modalityID =1,
   
   # Reader ID whose curves are drawn.
      readerID   =2)
```




# $\color{green}{\textit{Visualization }}$ of data and fitted model

##### FPF and TPF
 Recall that for the $C=5$ step rating,  FROC data is the pair $(H_c,F_c),c=1,2,...,5$  of hit $H_c$ for the $c$-th confidence level and false alarm $F_c$ with number of lesions $N_L$ and number of images $N_I$.

we define $C=5$ points as follows:
$$
(x_{c},y_{c}):=( \sum_{c\leq c' \leq C } F_{c^\prime}/N_{I}, \sum_{c\leq c' \leq C } H_{c^\prime}/N_{L}),
$$
where $c$ is the $c^{\text{th}}$ confidence level, and we call these points the pair of False positive fraction (FPF) per image and True positive fraction (TPT) per lesion.

In the following figure, the three circle means $(x_{c},y_{c}),c=1,2,3$ and the curve is the FROC curve defined later.



 `r paste0("![\" p value = 0.669,   bad fitting\"](",system.file("image", "bad.jpeg", package="BayesianFROC"),")")`




I hate the word FPF and TPF. I want to use the word _cumulative_ _false_ _positives_ (CFP) _per_ _image_ and _cumulative_ _true_ _positives_ (CTP) _per_ _lesion_. In my R program, I use CFP, CTP rather than FPF, TPF.




To tell the truth, I like the word cumulative false positives per images rather than FPF. If is my original word, but it means obvious and clear rather than FPF. Also I like the word cumulative true positives per lesions. The abbreviations of these two words are CFP and CTP in my paper.





Plotting these $C$-points in $(x,y)$-plain, we can visualize the FROC data. Connecting these points by line, we obtain the _empirical_ _FROC_ _curve._

##### FROC  curve and AFROC curve


We shall give a definition of FROC curve.



















#   $\color{green}{\textit{What is FROC curve?  }}$




 - FROC curve is alternative of ROC curve.
 - Unfortunately, area under the FROC curve is not finite, thus we deform the FROC curve so that the area under the curve is finite. The deformation gives us the notion of AFROC curve whose area under the curve is finite.


## _Pre_ FROC curve

###### Definition of pre FROC curve.

To define the FROC curve, we first define _pre_ FROC curve.

 
Define 

$y(t):=\mathbb{P}[Y>t]$ and $x(t):=\mathbb{P}[X>t]$.

Then we obtain the  _pre_ _FROC_ _curve_:

$$
\mathbb{R} \longrightarrow \mathbb{R}^2 \\
 t \mapsto (x(t),y(t))
$$


###### Expression of pre FROC curve.


To calculate $y(t):=\mathbb{P}[Y>t]$ and $x(t):=\mathbb{P}[X>t]$ more explicitly, we recall that 

$$X \sim \text{Normal}(0,1) \\ Y \sim \text{Normal}(\mu,\sigma^2)$$
, which is sometimes called the bi-normal assumption. Note that if $\mu$ is far from the mean of $X$ (that is 0), then the observer performance will be greater, since the noise and signal are well separated each other.

Please keep in mind that the parameter $\mu,\sigma^2$ is a part of parameter of our model which should be estimated.

 Since $\frac{Y -\mu}{\sigma}$ is distributed by the standard Gaussian, we can calculate the probability of the event that $\frac{Y -\mu}{\sigma} > \frac{t -\mu}{\sigma}$ and it follows that 
 
 $$y(t):=\mathbb{P}[Y>t] = \mathbb{P}[\frac{Y -\mu}{\sigma} > \frac{t -\mu}{\sigma}] = 1-\Phi( \frac{t -\mu}{\sigma}) $$

On the other hand,
$$x(t):=\mathbb{P}[X>t]   = 1-\Phi( t),$$
thus 
$$t = t(x) = \Phi ^{-1} (1-x(t) ).$$
Substituting this, we get 
 $$y(t) = 1-\Phi( \frac{t -\mu}{\sigma})  =   1-\Phi( \frac{ \Phi ^{-1} (1-x(t) ) -\mu}{\sigma})$$

##### calculation for FROC curve.


In the FROC data, each hits and false alarms are counted for each confidence level.

 we introduced the notations, $N_L$, $N_I$, $H_c$, $F_c$, $C$. In the R console, these notations are represented by `NL, NI, h, f, C`.
 
 If $C=5$, then the dataset for FROC analysis is the follows;
 
 
 Confidence Level    | No. of Hits | No. of False alarms
 :-----|----------|-------------
5 = definitely present |  $H_{5}$ |  $F_{5}$ 
4 = probably present   |  $H_{4}$ |  $F_{4}$ 
3 = equivocal          |  $H_{3}$ |  $F_{3}$ 
2 = probably absent    |  $H_{2}$ |  $F_{2}$ 
1 = questionable       |  $H_{1}$ |  $F_{1}$ 


We divide the set of 1 dimensional real numbers into $z_1 < z_2 < .... < z_C$.
Combining these thresholds with the parameter of the bi-normal assumptions  $\mu,\sigma^2$ , we obtain our model parameter. Other model parameters, for example $\lambda_c$ can be deduced from thresholds by $\lambda _c = -\log \Phi({z_c})$.





And  we consider the hits rate are generated by 
 $$p_{c}
 := \Phi (\frac{z_{c +1}-\mu_{}}{\sigma_{}})-\Phi (\frac{z_{c}-\mu_{}}{\sigma_{}})=\mathbb{P}[z_c <Y_l<z_{c+1}], $$  

We also define the sequence  $\lambda_1 < \lambda_2 < .... < \lambda_C$ such that 
$$F_c+...+F_C \sim \text{Poisson}(\lambda_cN_I)$$
,where $N_I$  is the number of images. This assumption is called the Poisson assumption in FROC context. The following condition combine the bi-normal assumption and the Poisson assumption:

$$ \mathbb{P}[X < z_1]= \mathbb{P}[F_1+F_2+F_3+...+F_C =0]\\  
\mathbb{P}[X < z_2]= \mathbb{P}[F_2+F_3...+F_C =0]\\ 
.....\\
\mathbb{P}[X < z_c]= \mathbb{P}[F_c+...+F_C =0]\\ 
...\\
\mathbb{P}[X< z_C]= \mathbb{P}[ F_C =0]\\ 
$$
which is equivalent that 

$$\Phi(z_c) = e^{-\lambda_c}$$ 

for all $c$.


Or equivalently,

$$1-x(z_c) = e^{-\lambda_c}$$ 






Thus we can get a correspondence between Poisson rate $\lambda$ and the threshold $z$.



Recall that FROC curves parameter $t$ is the range of the Gaussian random variable, we use $z$ instead of  $t$, then the curve is 
 $$y(t) =y(z)= 1-\Phi( \frac{t -\mu}{\sigma})  =   1-\Phi( \frac{ \Phi ^{-1} (1-x(t) ) -\mu}{\sigma})\\ 
  =   1-\Phi( \frac{ \Phi ^{-1} (1-x(z) ) -\mu}{\sigma})\\ 
    =   1-\Phi( \frac{ \Phi ^{-1} (e^{-\lambda} ) -\mu}{\sigma})\\ $$
In FROC analysis, the parameter of the FROC curve is taken by $\lambda$.




Where the last equality, we use the relation $1-x(z) = e^{-\lambda}$ which is the analogy ${}^{\dagger}$ (see Appendix) of $1-x(z_c) = e^{-\lambda_c}$.
 



Thus we can obtain the FROC curve as the following;

##### Definition of FROC curve

FROC curve is defined by

$$
\mathbb{R} \longrightarrow \mathbb{R}^2 \\
 \lambda \mapsto (\lambda,y(\lambda) )
$$

where $y(\lambda) = 1-\Phi( \frac{ \Phi ^{-1} (e^{-\lambda} ) -\mu}{\sigma})$ and $\mu,\sigma$ are parameter representing the mean and standard deviation of signal distribution.



## Property of the FROC curve

$$ y^{\text{FROC}}(\lambda):= 1-\Phi( \frac{ \Phi ^{-1} (e^{-\lambda} ) -\mu}{\sigma})\\
x^{\text{FROC}}(\lambda) := \lambda
$$

Note that FROC curve can be interpreted as the curve of the Expectation of the pair of TPF and FPF, where we use two abbreviations FPF = False Positive Fraction and TPF = True Positive Fraction. These words are widely used in the ROC theory and thus we omit the definition.


In mathematical philosophy, the expression is not important. The most important thing in mathematical expression is the property. So, in the case of the definition of the notion of the FROC curve, the above equations to represents the FROC curve is not important, so, reader may forget it !! I also forget the expressions. However we never forget about the property that the FROC curve is expectation pair of the FPF and TPF.
That is, we can write that 

$$
\mathbb{E}[ \sum _{ c \leq c' \leq C   }^C H_{c'} ] = y^{\text{FROC}}(\lambda_c) \\
\mathbb{E}[ \sum _{ c \leq c' \leq C}^C F_{c'} ] = x^{\text{FROC}}(\lambda_c) \\
$$

for all $c=1,2,...,C$.
I think this equality is more important than the expression of the definition of  FROC curve.




 
###### Appendix:   $1-x(z) = e^{-\lambda}$ which is the analogy ${}^{\dagger}$



In this section, we explain where this equality comes from.
From the previous section we use the discrete rating $1,2,...,c,...,C$. But now we shall consider __continuous__ case. Since continuous case contains discrete case, once we obtain the $1-x(z) = e^{-\lambda}$ for continuous case, then we can use it in discrete rating case. We shall denote reader's continuous rating by $\gamma \in \mathbb{R}$ instead of $c =1,2,...,C$ for discrete case. For example, $F_\gamma$ means the number of false positives with confidence level is greater than $\gamma$. Similarly,  $H_\gamma$ means the number of false positives with confidence level is greater than $\gamma$. So, if we denote the number of false positives with confidence level $\gamma$ by $f_\gamma$, then we may write the cumulative false positives per image as

$$
f_\gamma = \int_\gamma ^\infty F_\gamma d \gamma/N_I,
$$
which is analogy of the discrete case:
$$
 \frac{F_c+F_{c+1}+...+F_C}{N_I} 
$$
where $N_I$ denotes the number of images.


The analog of 
$$F_c+...+F_C \sim \text{Poisson}(\lambda_cN_I)
$$

is


$$
\int_\gamma ^\infty F_\gamma d \gamma\sim \text{Poisson}(\lambda _\gamma N_I)
$$

What is the analog of the followings discrete case;

$$ \mathbb{P}[X < z_1]= \mathbb{P}[F_1+F_2+F_3+...+F_C =0]\\  
\mathbb{P}[X < z_2]= \mathbb{P}[F_2+F_3...+F_C =0]\\ 
.....\\
\mathbb{P}[X < z_c]= \mathbb{P}[F_c+...+F_C =0]\\ 
...\\
\mathbb{P}[X< z_C]= \mathbb{P}[ F_C =0]\\ 
$$

It is 

$$
\mathbb{P}[X < z_ \gamma] = 
\mathbb{P}  [\int_\gamma ^\infty F_\gamma d \gamma  \neq0 ]
$$

or equivalently


$$
\mathbb{P}[X < z_ \gamma] = 
1- \mathbb{P}  [\int_\gamma ^\infty F_\gamma d \gamma =0 ],
$$
From which, we obtain 


 $$1-\Phi(z_ \gamma) = e^{-\lambda_ \gamma}$$.
 
 
 for all $\gamma$ and hence 
 
 
  $$1-\Phi(z) = e^{-\lambda}$$.

 
 
 In the previous notation, it is 
 
 $$1-x(z) = e^{-\lambda}.$$


















# $\color{green}{\textit{Statistical Test }}$ 
 
##### Bayesian ANOVA via Bayes factor
__Reference__:
 _"A Default Bayesian Hypothesis Test for ANOVA Designs" _


Ruud Wetzels, Raoul P.P.P.Grasman, and Eric-Jan Wagenmakers



 `r paste0("![\"X-ray of my teeth! I cannot understand why the result is not desired one, so  I have to say this program in under construction !\"](",system.file("image", "a.jpg", package="BayesianFROC"),")")`
 
 We can test the following null hypothesis for an FROC data in case of the multiple reader and multiple modality:

$$
H_0: \text{All modalities are same.}
$$


##### What is Bayesian ANOVA?

 1) Make a null hypothesis $H_0$ and its alternative $H_1$. 
 2) Make two models $\mathcal{M}_0$ constructed under $H_0$ and $\mathcal{M}_1$ made under $H_1$.
 3) Model selection based on WAIC or Bayes factor
 4) If $\mathcal{M}_1$ is selected, then  the null hypothesis $H_0$ is rejected.

Note that p value are not used.


##### Bayesian ANOVA is problematic

E.g.,

 - Under $H_0$ there are many models $\mathcal{M}_0$,$\mathcal{M}_0',$,$\mathcal{M}_0''$,$\mathcal{M}_0'''$,...
 
 Similarly,
 
  - Under $H_1$ there are many models $\mathcal{M}_1$,$\mathcal{M}_1',$,$\mathcal{M}_1''$,$\mathcal{M}_1'''$,...




but .... you know.. frequentist statistical test is also problematic.

 - p value monotonically decreases with respect to sample size.


 
##### _Null_ Hypothesis Model Vs _Alternative_ Hypothesis Model

In this package, there are two hierarchical Bayesian model. One is constructed under the null hypothesis that all modalities are same, and the another is constructed the alternative hypothesis that at least one modality is not same with the other modality, and we call the former the _null_ _hypothesis_  _model_ or simply the _null_ _model_, the latter the _alternative_ _model._
By comparing  the null model and alternative model by Bayes factor, we can test the null hypothesis. 

The alternative model is same the model which is already shown in the previous section. 

In the following we show the null model.


##### Null Hypothesis model




\begin{eqnarray*}
H_{c,m,r} & \sim &\text{Binomial }( p_{c,m,r}, N_L ),\\
F_{c,m,r} &\sim& \text{Poisson }( ( \lambda _{c} - \lambda _{c+1})N_L ),\\
\lambda _{c}& =& - \log \Phi (z_{c }),\\
p_{c,m,r}
&:=&\Phi (\frac{z_{c +1}-\mu_{m,r}}{\sigma_{m,r}})-\Phi (\frac{z_{c}-\mu_{m,r}}{\sigma_{m,r}}), \\
p_C & =& 1-\Phi (\frac{z_{C}-\mu_{m,r}}{\sigma_{m,r}}),\\
F_{C,m,r}  & \sim &\text{Poisson } ( (\lambda _{C} - 0)N_I),\\
A_{m,r}&:=&\Phi (\frac{\mu_{m,r}/\sigma_{m,r}}{\sqrt{(1/\sigma_{m,r})^2+1}}), \\
A_{m,r}&\sim&\text{Normal} ( \color{red}{A},\sigma_{r}^2), \\
dz_c&:=&z_{c+1}-z_{c},\\
dz_c, \sigma_{m,r} &\sim& \text{Uniform}(0,\infty),\\
z_{c} &\sim& \text{Uniform}( -\infty,100000),\\
\color{red}{A} &\sim& \text{Uniform}(0,1).\\
\end{eqnarray*} 
Our new model has parameters $z_{1}, dz_1,dz_2,\cdots, dz_{C}$, $A$, $\sigma_{r}$, $\mu_{m,r}$, and $\sigma_{m,r}$.



The R scripts for fitting the null model.
```{r,eval=FALSE}
fit <- fit_Bayesian_FROC( 
  
  #MCMC iteration
  ite  = 1111,
  
  #verbose summary or not
  summary = FALSE,  
  
  # number of MCMC chains 
  cha = 1,
  
  # Here we select the null model !!
  Null.Hypothesis = TRUE, 
  
  # example MRMC dataset
  dataList = dd 
  )
```




##### Alternative hypothesis model
The following model is the same in the previous sections, so we reprint here;

\begin{eqnarray*}
H_{c,m,r} & \sim &\text{Binomial }( p_{c,m,r}, N_L ),\\
F_{c,m,r} &\sim& \text{Poisson }( ( \lambda _{c} - \lambda _{c+1})N_L ),\\
\lambda _{c}& =& - \log \Phi (z_{c }),\\
p_{c,m,r}
&:=&\Phi (\frac{z_{c +1}-\mu_{m,r}}{\sigma_{m,r}})-\Phi (\frac{z_{c}-\mu_{m,r}}{\sigma_{m,r}}), \\
p_C & =& 1-\Phi (\frac{z_{C}-\mu_{m,r}}{\sigma_{m,r}}),\\
F_{C,m,r}  & \sim &\text{Poisson } ( (\lambda _{C} - 0)N_I),\\
A_{m,r}&:=&\Phi (\frac{\mu_{m,r}/\sigma_{m,r}}{\sqrt{(1/\sigma_{m,r})^2+1}}), \\
A_{m,r}&\sim&\text{Normal} ( \color{red}{ A_{m} },\sigma_{r}^2), \\
dz_c&:=&z_{c+1}-z_{c},\\
dz_c, \sigma_{m,r} &\sim& \text{Uniform}(0,\infty),\\
z_{c} &\sim& \text{Uniform}( -\infty,100000),\\
\color{red}{ A_{m} } &\sim& \text{Uniform}(0,1).\\
\end{eqnarray*} 
Our new model has parameters $z_{1}, dz_1,dz_2,\cdots, dz_{C}$, $A_{m}$, $\sigma_{r}$, $\mu_{m,r}$, and $\sigma_{m,r}$.


```{r,eval=FALSE}
fit <- fit_Bayesian_FROC( 
  
  #MCMC iteration
  ite  = 1111,
  
  #verbose summary or not
  summary = FALSE,  
  
  # number of MCMC chains 
  cha = 1,
  
  # Here we select the aleternative model !!
  Null.Hypothesis = FALSE, 
  
  # example MRMC dataset
  dataList = dd 
  )
```




##### _Distinction_ of _null_ model and _alternative_ model


The parameter $\color{red}{ A_{m} }$ are used in the alternative model instead of the parameter $\color{red}{ A}$ in the null hypothesis model.





 

 

### R script for ANOVA

In the following, we provide R scripts which implement the Bayesian ANOVA to test the null hypothesis that each modalities are same.



```{r,eval=FALSE}
#An example dataset for the case of Multiple readers and Multiple Modalities.
data  <- BayesianFROC::dataList.Chakra.Web




# Test the null hypothesis

Test_Null_Hypothesis_that_all_modalities_are_same(data,ite = 1111)

# where ite is the iteration of MCMC to create two stanfit object, one is the null model and the another is the alternative model.

```


The internal of the code, the functions in the package `bridgesampling` are running.

R often crashes when the codes run, sorry,... I do not know why ? It is heavy ??




#### Bayes Factor
#### R code for comparison of two objects of class `stanfit`


Let `fitH0` and `fitH0` be `stanfit` objects.
The following code compare these two objects by the Bayes factor.




```{r,eval=FALSE}


# method='WARP' if another algorithm (WARP-III) you want.
# silent is whether calculation are shown


# Null model 
H0 <- bridgesampling::bridge_sampler(fitH0, method = "normal", silent = TRUE)
print(H0)

# Alternative model  
H1 <- bridgesampling::bridge_sampler(fitH1, method = "normal", silent = TRUE)
print(H1)

# Test the Null hypothesis that all modalities are same. 
BF10 <- bridgesampling::bf(H1, H0)
print(BF10)

# If the number is greater, then we reject H0 with more confidence. 
```




#### Limitation (no need to read)

Our modeling begin from the hits and false alarms  obtained reducing the  primitive FROC data, which is more informative, and the so-called figure or merit (FOM) is calculated with more informative dataset instead of the author's dataset.

I have to say our hierarchical model is a toy. The starting point of this model has very problematic since the  data of hits and false alarms are obtained  by reducing the information of FROC trials. The Jafroc data formulation contains the more precisely information and the notion of figure of merit is obtained by using this precise information. Thus without changing our modeling starting point, our model have to be said toy. However, This is the first attempt in the world to analyze the FROC by Bayesian model, I think it is sufficient. This is only hobby, I am not a statistician nor radiologist nor academic post.  I am only unemployed dirty old man.  Good Luck for future research and my future. ..ha i have a ache in my arms, right arms, so I no longer write the R scripts or .. ha,....






# $\color{green}{\textit{Validation }}$  of our model


##### p value of $\chi^2$ goodness of fit

 `r paste0("![\"X-ray of my teeth!! Our MCMC sample contains bias ? \"](",system.file("image", "a.jpg", package="BayesianFROC"),")")`


First, we outline the *p value* of the *chi square goodness of fit statistics* for Bayesian version. In frequentist p values are calculated with data $D$ and an estimated parameter $\theta$ such as maximal likelihood estimates $\hat{\theta}(D)$, so we may write $\chi^2(D)=\chi^2(D|\hat{\theta}(D))$. However, in Bayesian context, the parameter of model is not deterministic, so, the p values in Bayesian context also depend on the parameter of model. To obtain the p values in deterministic way in Bayesian context, we integrate a p values by the posterior predictive distribution. The posterior predictive distribution $\pi(D|D_0)$ for given data $D_0$ is defined by the posterior mean of the likelihood;

$$\pi(D|D_0) := \int f(D|\theta)\pi(\theta|D_0)d\theta \times \text{const}
$$

For the detail of p value, please see the Gelman’s book “ *Bayesian Data Analysis*”.

The package *BayesianFROC* implement the calculation of the posterior probability p value for the chi square goodness of fit statistics for a single reader and single modality FROC model.

In this vignette, we explain how to run the function which calculate the posterior predictive p value. 

 - Prepare data in case of a single reader and single modality
 - Get  *_fitted model object_* of class *_stanfitExtended_* by fitting model to data via the function `fit_Bayesian_FROC()` in this package.

 - Calculate the posterior predictive p value via `p_value_of_the_Bayesian_sense_for_chi_square_goodness_of_fit()`
 
 
 So all we have to do is simple, that is, merely run only two functions.

- `fit_Bayesian_FROC()`
- `p_value_of_the_Bayesian_sense_for_chi_square_goodness_of_fit()`
 
The resulting object contained p value. 


Using pipe operator `%>%` from the package *stringr*, the whole code can be written as follows;


```{r,eval=FALSE}
# 1) First, attach a package stringr to use pipe operator %>% 
library(stringr)

# 2) Prepare data
data <- dataList.Chakra.1

# 3) Using the pipe operator we can say the work follow code by only 1 row.
data %>%  fit_Bayesian_FROC() %>% p_value_of_the_Bayesian_sense_for_chi_square_goodness_of_fit()
 
```

 
## Example Code to Calculate the Posterior Predictive P value



```{r,eval=FALSE}

# 1) Prepare a dataset
dat <- BayesianFROC::dataList.Chakra.1


# 2) Fitting
fit <- BayesianFROC::fit_Bayesian_FROC(dat)


# 3) Calculation of the P value 
p_value_of_the_Bayesian_sense_for_chi_square_goodness_of_fit(fit)

```


##### Outputs of `p_value_of_the_Bayesian_sense_for_chi_square_goodness_of_fit()`

-  p value
-  Chi squares
 
 In the following, we shall show what the it means in the printed result in the R console when we execute the above code to evaluate p-value. 
 
 
```{r,eval=FALSE}

|the 7984-th |                          6.440|                            12.00|TRUE                    |
|the 7985-th |                          0.857|                            13.00|TRUE                    |
|the 7986-th |                          1.670|                            13.50|TRUE                    |
|the 7987-th |                          3.880|                            12.20|TRUE                    |
|the 7988-th |                          1.300|                            16.20|TRUE                    |
|the 7989-th |                          6.190|                            11.40|TRUE                    |
|the 7990-th |                         10.900|                            12.60|TRUE                    |
|the 7991-th |                          3.360|                            12.80|TRUE                    |
|the 7992-th |                          5.110|                            14.50|TRUE                    |
|the 7993-th |                          5.530|                            12.90|TRUE                    |
|the 7994-th |                          4.430|                            13.00|TRUE                    |
|the 7995-th |                          3.570|                            14.70|TRUE                    |
|the 7996-th |                          9.890|                            17.70|TRUE                    |
|the 7997-th |                         72.200|                             9.40|FALSE                   |
|the 7998-th |                          2.140|                            10.40|TRUE                    |
|the 7999-th |                          7.750|                             9.95|TRUE                    |
|the 8000-th |                          3.310|                            15.20|TRUE                    |

*  Note that the posterior predictive p value is a rate of TRUE in the right column in the above table. 


 The p value of the posterior predictive measure for the chi square discrepancy. 
                                                                        0.916375 
```

The last number `0.916375 ` is the desired p value of the chi square in the Bayesian context according to Gelmann's book.


 - The first column means the *__sample__* *__ID__* of the *Hamiltonian Monte Carlo Simulation* without warming up. 
 - The second column means the value of the chi square with _observed_ data and the corresponding MCMC sample.
 - The Third column means the value of the chi square with _replicated_ data and an MCMC sample, I forget,
 - The fourth column means the inequality that the second column < the third column. So these inequality is required for calculating p value of the Bayesian seance according to Gelmann's book.
 - More `TRUE` in the 4-th column means  *better* fitting model to data
 - p-value is proportion to the number of `TRUE` in the 4-th column.






Running the above R script, the graphic devices appears and shows the replicated FROC data from the posterior predictive distribution are drawn. The sample are used to calculate the p-value in the Bayesian sense, we needs samples form the* posterior predictive distribution*. 

 
















## Example of __p__ __value__ for $\chi^2$ goodness of fit


Note that p-value is depend on only data. So, odd data cause the small p value, which will rejects the _null_ _hypothesis_ that fitting is good.


##### Example;  $\color{red}{\textit{ Good  }}$ fitting to data `dataList.Chakra.1`

Intuitively, all data points drawn by circle are on FROC curve, thus we may say   $\color{red}{\textit{ good  }}$ fitting. $\color{red}{\textit{P value is 0.925. }}$ 

 `r paste0("![\" p value = 0.925,  good fitting\"](",system.file("image", "FROCcurve.jpeg", package="BayesianFROC"),")")`

###### Example; $\color{red}{\textit{ Bad  }}$ fitting to data `BayesianFROC::dataList.High`

Intuitively, two data points drawn by circle are _not_ on FROC curve, thus we may say  $\color{red}{\textit{ bad  }}$ fitting. $\color{red}{\textit{P value is 0.669. }}$ which is smaller than the above 0.925, indicating fitting is _not_ better than above.

 `r paste0("![\" p value = 0.669,   bad fitting\"](",system.file("image", "bad.jpeg", package="BayesianFROC"),")")`


###### validation of p value calculator `p_value_of_the_Bayesian_sense_for_chi_square_goodness_of_fit`

$\color{blue}{\textit{The above two examples show that p value is }} \color{red}{\textit{ compatible  }} \color{blue}{\textit{with our intuition. }}$  






##### Appendix: Theory of P value in Bayesian sense



In the following, we shall use the general notation.
Let $y_{\text{obs}}$ be observed data and $f(y|\theta)$ be a model (likelihood) for a future data $y$. We write a prior and a posterior distributions by $\pi(\theta)$ and $\pi(\theta|y) \propto f(y|\theta)\pi(\theta)$. The posterior predictive distribution  is defined by $p(y|y_{\text{obs}}) := \int f(y|\theta)\pi(\theta|y_{\text{obs}}) d\theta$


In our case, the data $y$ is a pair of hits and false alarms, that is $y=(H_1,H_2, \dots H_C; F_1,F_2, \dots F_C)$ and $\theta = (z_1,dz_1,dz_2,\dots,dz_{C-1},\mu, \sigma)$. We shall define the $\chi^2$ discrepancy ( goodness of fit statistics ) to validate that the model fit the data.

\[
T(y,\theta) := \sum _{c=1,\cdots,C} \biggr( \frac{[H_c-N_L\times p_c]^2}{N_L\times p_c}+
\frac{[F_c-(\lambda _{c} -\lambda _{c+1} )\times N_{I}]^2}{(\lambda _{c} -\lambda _{c+1} )\times N_{I} }\biggr).
\]

for single reader and single modality case.

\[
T(y,\theta) := \sum _{r=1, \cdots,R} \sum _{m=1,\cdots,M} \sum _{c=1,\cdots,C} \biggr( \frac{[H_{c,m,r}-N_L\times p_{c,m,r}]^2}{N_L\times p_{c,m,r}}+
\frac{[F_{c,m,r}-(\lambda _{c} -\lambda _{c+1} )\times N_{L}]^2}{(\lambda _{c} -\lambda _{c+1} )\times N_{L} }\biggr).
\]

for multiple reader and multiple case, where the false positive fraction is per lesion istead of per image.

Note that $p_c$ and $\lambda _{c}$ depend on $\theta$.
In this statistic, the number of degrees of freedom is $C-2$.




Classical frequentist methods, the parameter $\theta$ is a fixed estimates, e.g. the maximal likelihood estimator, however, in Bayesian context, the parameter is not deterministic, so, by integrating with the posterior predictive distribution, we can get the posterior predictive $\chi^2$ value and its p-value. Let $y_{\text{obs}}$ be observed data. Then the posterior predictive p value is defined by

\[
p \text{ value of $y_{\text{obs}}$}  = \int \int dy d\theta I_{T(y,\theta) > T(y_{\text{obs}},\theta)}f(y|\theta)\pi(\theta|y_{\text{obs}})
\]



In the following, we show how to calculate the double Integral. Suppose that  $\theta _1, \theta_2,\cdots,\theta_N$ are samples from the posterior distribution via Hamiltonian Monte Carlo simulation, we obtain a sequence of models (likelihoods) ;  $f(y|\theta_1),f(y|\theta_2),\cdots, f(y|\theta_N)$. 
 Drawing the samples $y_1,y_2,\cdots, y_N$ so that each $y_i$ is a sample from the distribution whose density function is $f(y|\theta_i)$, 
and it is desired one, that is, we can interpret that the samples $y_1,y_2,\cdots, y_N$ is drawing from the posterior predictive distributions.  Using the law of large number, we can calculate the noble integral of the p value by 



\[
p \text{ value}  =\frac{1}{N} \sum I_{T(y_i,\theta_i) > T(y_{\text{obs}},\theta_i)}
\]

 









##### Validation: Replicate various data-sets from  a fixed true distribution


 `r paste0("![\"X-ray of my teeth!!\"](",system.file("image", "a.jpg", package="BayesianFROC"),")")`
 
 
I implement whether estimates contains bias or not.
But, now, I exclude the dataset that the its fitting has not pass the R hat criterion, so I have to say the bias is calculated in convergence case only, and it is not suitable, so I will exclude it. But program  shows the convergence rate and almost all procedure had converged, so the exclusion  has not strong effect on the bias calculation.


##### Validation: A function for validations.

For the user it is sufficient to know only one function `validation.dataset_srsc_for_different_NI_NL()`.
This function replicate many data-sets from known distributions which user specified before execution.
In FROC data, the number of lesions and images corresponds samples size. So, large number of image gives us more small bias which will be confirmed by the function `validation.dataset_srsc_for_different_NI_NL()`.

The following, the important variables of `validation.dataset_srsc_for_different_NI_NL()` are shown.

- `NLvector`: A vector, whose each component means No. of lesions. We fit models to the replicated data with these fixed lesions specified this vector `NLvector` .  For, example, if `NLvector=c(100,1000)` then the first we replicate various hits and false alarms under the assumption of 100 lesions, and evaluates the bias. Next we do same but changing 1000 lesions instead.  
-  `ratio`: The ratio of image:lesions. It bother for me to input both the images and lesions. Thus I construct so that it is sufficient to input only one of them, that is if lesion is specified, then the number of images are automatically generated by satisfying this `ratio`
-  `replicate.datset`: For fixed number of lesions, images, the dataset of hits and false alarms are replicated, and the number of replicated data sets are specified by this variable.
-  `mean.truth`: The mean of signal distribution.
-  `sd.truth`: The standard deviation of signal distribution.
-  `z.truth`: The threshold of the bi-normal assumption
-  `ite`: The Hamiltonian Monte Carlo iterations used in each fitting for replicated data-sets.


 

 
 
 
 
 
#### Output: Error of estimates calculated by replicated data-sets

Each number in table means the errors of parameter, i.e., the mean values of estimates minus true parameter over all replications. We can see that the number of images and lesions become more large, then these error tends to zero. That is, in FROC models, the number of lesions and images corresponds sample size, so if these values are larger, then the hits and false alarms also become larger, and it leads us to smaller biases. 

           validation.dataset_srsc_for_different_NI_NL()
  
     
|Name.of.Parameters |  1-th model|    2-th model|   3-th model|  4-th model|
|:------------------|-----------:|-------------:|------------:|-----------:|
|Number of Images   | 200.0000000| 20000.0000000|  2.00000e+06|  2.0000e+08|
|Number of Lesions  | 100.0000000| 10000.0000000|  1.00000e+06|  1.0000e+08|
|z[ 1 ]             |   0.1055186|     0.0103358|  1.01020e-03|  1.0720e-04|
|z[ 2 ]             |   0.1956183|     0.0132404|  1.20850e-03|  1.3030e-04|
|z[ 3 ]             |   1.1307551|     0.0309471|  2.52580e-03|  2.9210e-04|
|mean.of.signal     |  -0.4843169|    -0.0463222| -4.95890e-03| -4.2730e-04|
|sd.of.signal       |   3.8036830|     0.1160011|  1.07218e-02|  1.0843e-03|
|AUC                |  -0.0348429|    -0.0041814| -4.49200e-04| -4.0100e-05|
|dz[ 1 ]            |   0.0900996|     0.0029046|  1.98300e-04|  2.3100e-05|
|dz[ 2 ]            |   0.9351368|     0.0177067|  1.31730e-03|  1.6170e-04|
|p[ 1 ]             |  -0.0384759|    -0.0019671| -1.90400e-04| -1.9200e-05|
|p[ 2 ]             |  -0.0119591|    -0.0015342| -1.72000e-04| -1.5200e-05|
|p[ 3 ]             |  -0.0121365|    -0.0026324| -2.75400e-04| -2.5300e-05|
|lambda[ 1 ]        |  -0.0384759|    -0.0019671| -1.90400e-04| -1.9200e-05|
|lambda[ 2 ]        |  -0.0119591|    -0.0015342| -1.72000e-04| -1.5200e-05|
|lambda[ 3 ]        |  -0.0121365|    -0.0026324| -2.75400e-04| -2.5300e-05|
 























#### Simulation Based Calibration (SBC);Under construction

This is a statistical test for the null hypothesis that the MCMC sampling contains no bias.

From SBC algorithm runs, then the pseudo uniform distribution are obtained.

 - if it is exactly uniform distribution, then we cannot say that the MCMC contains bias,

 - If pseudo uniform distribution is differ from uniformity, then we reject the null hypothesis and we may consider that MCMC sample contains bias. 


# Appendix

##### Appendix: Terminology

 - *__modality__*: Imaging methods such as MRI, CT, PET, ..., etc
 - *__image__*: radiography contains signal and noise. In Randiological context, signal is nodule or lesion.
 - *__lesion__*: signal which should be detected by reader, it is also called nodule, diseased-case...etc
 - *__hits__*: True positive; TP
 - *___False___* *__alarm__*: False positive; FP
 - *__CFP__*:  Abbreviation of  cumulative false positive per image (or per lesion) which is the author's word, it  - customary called False Positive Fraction or False positive per Image (FPI) in FROC context.
 - *__CTP__*:  Abbreviation of Cumulative True Positive per lesion, note that signal is each lesion, thus this cannot be said per image.
 - *__FROC__*: Abbreviation of Free-response ROC. The difference of ROC and FROC is that ROC allows only dichotomous answer whether the image is diseased or not. On the other hand, in FROC task, each reader can answer multiple answer for each image. This multiple answer for one image cause the multiple False alarms for each image.
 - *__FROC__* *__curve__*: Provides an alternative of ROC curve, so interpretation of FROC curve is same as ROC curve, that is, if FROC curve is more upper convex, then the observer performance is better.  This curve fit to the FPF and TPF points.
 - *__AFROC__ __curve__*: Abbreviation of Alternative FROC curve. Since FROC curve is not bounded, the area under the FROC curve is infinity and thus cannot be defines. Indefiniteness of AUC for FROC curves leads us to the AFROC curve contained in the bounded region, and it provides the AUC for FROC trial.
 - *__Bi-normal__* *__assumption__*: Two Gaussian distribution are used to specify the hit rate and false alarm rate.
 - *__Poisson__* *__assumption__*: It determined the false alarm rate, required some compatibility condition with the Bi-normal assumptions.
 - *__AUC__*: Area under the AFROC curve.

##### Appendix: 2

##### Appendix: Basic words


 Basic words |Truth = Positive  | Truth = negative
 :-----|:----------:|:-------------:
Reader's positive|   TP |  FP 
Reader's  negative   |FN |    TN 
 

We only focus TP and FP, thus we call it **_hit_** and **_false_** **_alarms_**, respectively. That is:


 Basic words |Truth = Positive  | Truth = negative
 :-----|:----------:|:-------------:
Reader's positive|   <span style="color:red">**_hit_** </span>| <span style="color:red"> **_False_** **_alarm_** </span>
Reader's  negative     |FN |    TN 

 - Number of hits are denoted by `h` and  number of false alarms are denoted by `f` in the R console, respectively.

 -   Number of hits are denoted by $H$ in TeX and  number of false alarms are denoted by $F$ in TeX.


##### Appendix (:' 3)

In this section, I show the most important things.

I worked some company, then as a work, someone let me use some washing soap, whose component is 

 - Alkyl ether sulfate sodium ester (AES),
 - Alkyl benzyl dimethyl ammonium salt
 - and others.
 
Then, when I exposed it for (4 h; 1 year 18 month ago), then I felt Irritated stimulus in my whole body, shock pains. Then my body become bad, especially, my skin shows my pain as an prurigo nodularis. Now I am prescribed the following steroid of rand _very_ _strong_ and protopic and Bilanoa tablet.

I regret to use it, since in my private life, I never have used such things.

Reader should not use such a things. My all body, brain, eyes, teeth, leg, hands, breathing, elbow, finger, anus, karasuma, scalp, blood vessels, thinning hair, .... etc.

Irritated stimulus is very strong fist month. Especially, in brain, the stimulus continue 10 months. Now, 1 year and 18 months passed from the exposure, but my body is still very bad.

The traces of prurigo nodularis make me very sad.

So, we should not use any petroleum detergent, causes environmental hormone, very strong effect on our body.

The most important things are _bio_ _surfactant._ We should establish the technology of bio surfactant.




 


##### Appendix: These model are made in very past...
2017 May ~ 2017 August, the author had made theses Bayesian FROC models.

For detail, please see my pre-print on Arxiv:

" _Bayesian_ _Models_ _for_ _free_- _response_ _receiver_ _operating_ _characteristic_ _analysis_"

##### Appendix; Divergent Transition

Note that we use the following definition

$$p_{C}= 1-\Phi (\frac{z_{c}-\mu}{\sigma}), $$

instead of 

$$p_{C}= \Phi (\frac{z_{C +1}-\mu}{\sigma}) -\Phi (\frac{z_{c}-\mu}{\sigma}), $$
with theoretically infinity but numerically very large number $z_{C+1}$.

One would think that this note is redundant, but this is very important to avoid the divergent transition issues which is a bias in MCMC sampling. The author first make a stan file with the latter, then divergent transition occurs. To avoid the divergent transitions, the centering is the one known methods, but in my case, I think it does not relates the centering. Now, my package implements the former.


In our model, author write down the highest confidence level separately. Because if not, then it cause the divergent transition issue. In the past, I wrote the model with the assumption that the highest threshold $z_{C+1}$ is the very large number (theoretically it is infinity) and whose prior is the uniform distribution with very large support, then it cause the divergent transitions almost all iterations in the MCMC. Also, I use the target formulation to avoid Jacobin warnings, such warnings also difficult for me to solve or understand what it say or how overcome. 


One may thinks my models are very complex. In fact many people will hate my explanation, my models. One reason why my model is so complex  is the FROC statistical model  is  very complex and without the reader's effort, cannot understand. Further more, the description of my model is changed to avoid the divergent transition issues, it make my model more complex one.
Further I had to overcome Jacobian issues, which formulation is not intuitive for me.

I am very tired to explain my model and also disappeared my ability :'-D

These model are made in two years ago, 2017.May ~ 2017. August.

I briefly explain the divergent transition in my model. In theoretical perspective, it natural to introduce the highest decision threshold as an very large number, which is a parameter of model. Unfortunately, it is distributed by the uniform distributions with very large support. Then once we introduce such  theoretically infinity parameter in the numerical program, it case the divergent transition issues.

##### Appendix: About the author

Hello!! Everyone !! I am around thirty years old. When I am 33? years old, that is 2017.12.28, I had been exposed to a cleanser about 4 hour, then my body become very poor physical condition and whole my body has the so-called prurigo nodularis. When I was exposed, then I felt a stimulating sensation, such as "tiktik". 
Now, I have Bilanoa, Tacrolimus, ANTEBATE. So,Exposure developed atopic dermatitis. Please be careful!! The exposure cause various pain 24 hours out of 24 all the time at all hours （of the day and night）



##### Appendix: Future research direction

 - WAIC or Bayes factor approach
 - My hierarchical model does not have finite WAIC, so why ? I have to show it, .. but ... I am tired.



#  Appendix:
### $\color{green}{\textit{ Addition of Poisson distributions }}$ 

 
If $X \sim \text{Poisson}(\lambda_X)$ and $Y \sim \text{Poisson}(\lambda_Y)$, then $X+Y \sim \text{Poisson}(\lambda_X + \lambda_Y)$. We use this relation in the above of the false alarm context.

#  Appendix:
### $\color{green}{\textit{ Calculation of Poisson distributions }}$

If some random variable $N$ satisfy $N \sim \text{Poisson}(\lambda)$ then $\mathbb{P}(N=0)= e^{-\lambda}$ (redundant ?).



##### Appendix
 `r paste0("![\"X-ray of my teeth!!\"](",system.file("image", "a.jpg", package="BayesianFROC"),")")`

I think for the users of this package the following definitions are no need to understand nor remember. To tell the truth, the author sometimes forget these definitions when I develop this packages or running this code. So, do not bother yourself even if you can not understand the following things. Especially, I think for the  medical researcher or radiologists it is difficult to understand. I studied mathematics (Differential Geometry) in some university, so it is very easy for me, but I do not want to force the users of this package to understand these precise definitions, and it is the reason why I put these in Appendix.



##### Appendix Notations:

$\Phi()$ denotes the cumulative distribution function of the Gaussian distribution with mean 0 and variance 1.
$\Phi^{-1}()$ is the inverse mapping of $\Phi()$.


##### Appendix Definition of FROC curves


Let $(x,y)$ be a Cartesian coordinate system on a plane. Then, for any non-negative real number $\lambda$, the FROC curve is defined as 
$$
(x(\lambda),y(\lambda)) :=  \bigl(\lambda , 1-\Phi(b\Phi ^{-1}(\exp(-\lambda)) -a ) \bigr) \in \mathbb{R}^2
$$
Or equivalently,

$$
 (x(\lambda),y(\lambda)) :=   \bigl(\lambda , \text{Prob} \bigr\{ Y > \Phi ^{-1}(\exp(-\lambda)) \bigl\} \bigr)  \in \mathbb{R}^2.
$$

where $Y$ is a signal distribution, $a =\mu/\sigma$ and $b=\sigma$.  
It is easy to see that these two definition is  same.

Note that AUC of FROC is infinity. So, in this package AUC means the next AFROC curve's AUC.

##### Appendix Definition of AFROC curves


Similarly, let $(\xi, \eta)$ be a Cartesian coordinate system for the square $0\leq \xi \leq 1, 0\leq \eta \leq 1$. Then, for any non-negative real number $\lambda$, we define an AFROC curve as
$$(\xi(\lambda), \eta(\lambda)) := \bigl(1-\exp(-\lambda) , 1-\Phi(b\Phi ^{-1}(\exp(-\lambda)) -a ) \bigr) \in \mathbb{R}^2.
$$

##### Appendix Definition of Cumulative false positives and true positives

we define $C$ points as follows:
$$
(x_{c},y_{c}):=( \sum_{c\leq c' \leq C } F_{c^\prime}/N_{I}, \sum_{c\leq c' \leq C } H_{c^\prime}/N_{L}) \in \mathbb{R}^2,
$$
where $c$ is the $c^{\text{th}}$ confidence level, and we call these points the _cumulative false positive-cumulative true positive (CFP-CTP) points over $(N_I, N_L)$_. 

In the  case of single reader and single modality, this is used if `ModifiedPoisson = FALSE` in `BayesianFROC::fit_Bayesian_FROC()`.
IF `ModifiedPoisson = FALSE` is true then the following points are depicted instead:

$$
(x_{c},y_{c}):=( \sum_{c\leq c' \leq C } F_{c^\prime}/N_{L}, \sum_{c\leq c' \leq C } H_{c^\prime}/N_{L}) \in \mathbb{R}^2,
$$

##### Appendix Another model for Multiple FROC curves


The previous hierarchical model draws FROC curves for each reader and each modality. On the other hand, the following model draw FROC curve for each modality, __not__ __depend__ __on__ __readers.__



The following model are implemented in the function `BayesianFROC::fit_MRMC_versionTWO()`.


 $$H_{c,m,r}   \sim  \text{Binomial}( p_{c,m,r}, N_L ),  $$
 $$F_{c,m,r}  \sim  \text{Poisson} ( ( \lambda _{c} - \lambda _{c+1})N_L ), $$ 
 $$\lambda _{c}  =  - \log \Phi (z_{c }),  $$
 $$p_{c,m,r}
 := \Phi (\frac{z_{c +1}-\mu_{m,r}}{\sigma_{m,r}})-\Phi (\frac{z_{c}-\mu_{m,r}}{\sigma_{m,r}}), $$  
 $$A_{m,r} := \Phi (\frac{\mu_{m,r}/\sigma_{m,r}}{\sqrt{(1/\sigma_{m,r})^2+1}}),  $$ 

 $$A_{m,r} \sim \text{Normal} (A_{m},\sigma_{r}^2),   $$
 $$\mu_{m,r} \sim \text{Normal} (\mu_{m},\sigma _{\mu,m}^2),   $$
 $$\sigma_{m,r} \sim \text{Normal} (\sigma_{m},\sigma _{\sigma,m}^2),   $$
$$  a_{m} :=\mu_{m} / \sigma_{m}     $$
$$  b_{m} := 1/ \sigma_{m}      $$
 $$dz_c := z_{c+1}-z_{c},  $$
 $$dz_c, \sigma_{m,r}  \sim  \text{Uniform}(0,\infty),  $$
 $$z_{c}  \sim  \text{Uniform}( -\infty,100000),  $$
 $$A_{m}  \sim  \text{Uniform}(0,1).  $$

#####  Appendix: Rjafroc 

This package is independent to *__Rjafroc__*, but relates it. But now 2019 May, the Rjafroc is not on CRAN, so I cannot link to Rjafroc.

In my program is completely independent of (on ? to ?) Rjafroc, so it is no problem whether Rjafroc is on CRAN or not.


##### Appendix: Thank you for your reading

I love you, thank you !! Good luck !! For any questions, send me a mail:

tsunoda.issei1111  _at_ gmail.com


I also want to find a job, if you want to work with me, then send me a mail !! Please,...now, I am a dirty old man without money .... oh shit !! Love and peace !!




##### Appendix: References:


 - Issei Tsunoda (2019?): Pre-print;  Bayesian Models for  free - response receiver operating characteristic analysis.

 - Chapman and Hall/CRC Bio-statistics Series;
Statistical Evaluation of Diagnostic Performance 
Topics in ROC Analysis
Chapter 8; pp. 163- 197


I think the following paper is sufficient to understand my paper or this package:

 - Dev Chakraborty (1989) \doi:10.1118/1.596358; Maximum likelihood analysis of free - response receiver operating characteristic (FROC) data.


##### Appendix: I did not think that ...
My package is everyday downloaded one more people, so I have to write the precise theory of my Bayesian Model, so I wrote this vignettes very slowly with honesty. I am happy if this vignette helps someone.










##### Appendix:  Discrete and Continuous rating is essentially same.

 `r paste0("![Radiograph](",system.file("image", "a.jpg", package="BayesianFROC"),")")`



Recall that the format of FROC data is the following table:
 
 Confidence Level    | No. of Hits | No. of False alarms
 :-----|----------|-------------
5 = definitely present |  $H_{5}$ |  $F_{5}$ 
4 = probably present   |  $H_{4}$ |  $F_{4}$ 
3 = equivocal          |  $H_{3}$ |  $F_{3}$ 
2 = probably absent    |  $H_{2}$ |  $F_{2}$ 
1 = questionable       |  $H_{1}$ |  $F_{1}$ 


Here, we use the confidence level as predictor and if we use some *__bio__*-*__marker__* instead of the above discrete rating and if bio marker is *not* discrete, then it corresponds *continuous* *rating.* Here, bio marker means, e.g., TSH/EULIA, FT4/EULIA, FT3/EULIA, T4/EULIA, T3, Baso, Eosino, Neutro, Stab, Seg, Lympho, Mono, MCV, MCHC, TP, A/G, TTT, ZTT, AST(GOT), ALT(GPT), gamma-GT, LD, LAP, CK, L/H, Na, K, CL, Ca, IP, Mg, Fe, CRP, RF, ASO, HbA1c(NGSP), RAST,  IgE, ..., etc. 




But, sequential rating reduce to the above table by dividing region such that bio-marker fall into some partition. Thus we may assume that the FROC data has the above format and it is independent whether rating is discrete or continuous.

 Rating (instead of Confidence Level)   | No. of Hits | No. of False alarms
 :-----|----------|-------------
0.8< biomarker < 1.0 |  $H_{5}$ |  $F_{5}$ 
0.6< biomarker < 0.8 |  $H_{4}$ |  $F_{4}$ 
0.4< biomarker < 0.6 |  $H_{3}$ |  $F_{3}$ 
0.2< biomarker < 0.4 |  $H_{2}$ |  $F_{2}$ 
0.0< biomarker < 0.2 |  $H_{1}$ |  $F_{1}$ 


Good bio-maker has coerce data such that the hits and false alarms has the following monotonicity conditions (not needed to satisfy precisely);

$$
H_1 < H_2 < H_3 < H_4 < H_5 \\
F_1 > F_2 > F_3 > F_4 > F_5 \\
$$

If data does not have these monotonicity, then the model fitting will be not better.

Another criterion whether bio marker is useful is that if the estimated AUC is far from 1/2 then the bio-marker is more important. Note that if bio-marker is less than 1/2, then by taking inverse order, we will obtain the bio-marker which is greater than 1/2. Thus bio-marker is significantly important if and only if the _absolute_ value of (AUC - 1/2) is more greater.

So, in FROC context, the rating is not required the confidence level, but for simplicity and confidence level is always significant, thus we use in the general theory.

So, the word "rating" is more general than "confidence level".

Note that confidence level cannot be said good predictor, since each reader has their trend to take what confidence level he or she use under the FROC trial. So, some reader use confidence level only the lowest and highest one, and such usage of confidence rating mislead our model or estimates. So, to avoid such bias caused reader, we sometimes recommend to use bio marker instead confidence level for the rating.

Anyway the most important thing is that rating is a more general notion than the confidence level.



If there are several bio markers, then we need to reduce them into one dimensional characteristic such as linear combinations of them and this reduced characteristic, we can examine their effect to the observer performance, but ...


 
















#  Appendix:
### $\color{green}{\textit{ Why a Bayesian approach now. }}$



In the following, the author pointed out why frequentist p value is problematic. Of course, under some condition, Bayesian p -value coincides with frequentist p value, so the scheme statistical test is  problematic. We shall show the reason in the following simple example.

To tell the truth, I want to use epsilon delta manner, but to read not mathematics people, I do not use it.

######  Monotonicity issues on p value
 
The methods of statistical testing are widely used in medical research. However, there is a well-known problem, which is that a large sample size gives a small p-value. In this section, we will provide an explicit explanation of this phenomenon with respect to simple hypothesis tests. 





Consider the following null hypothesis $H_0$ and its alternative hypothesis $H_1$;
\begin{eqnarray*}
H_0: \mathbb E[ X_i] &=&m_0, \\
H_1: \mathbb E[ X_i] &>&m_0, \\
\end{eqnarray*}
where $\mathbb E[ X_i]$ means the expectation of random samples $X_i$ from a normal distribution whose variance $\sigma _0 ^2$ is known. In this situation, the test statistic is 
given by 
\[ 
Z^{\text{test}} := \frac{\overline{X_{n}} -m_0 }{\sqrt{\sigma _0 ^2/n}}, 
\]
where $\overline{X_{n}} := \sum_{i=1,\cdots,n} X_i /n$ is normally distributed with mean $m_0$ and standard deviation $\sigma_0/\sqrt{n}$.
Under the null hypothesis, $Z^{\text{test}}$ is normally distributed with mean $0$ and a standard deviation $1$ (standard normal distribution).
The null hypothesis is rejected if $Z^{\text{test}} >z_{2\alpha}$ , where $z_{2\alpha}$ is a percentile point of the normal distribution, e.g., $z_{0.025}=1.96 .$





Suppose that the true distribution of $X_1, \cdots, X_n$ is a normal distribution with mean $m_0 + \epsilon$ and variance $\sigma _0 ^2$, where $\epsilon$ is an arbitrary fixed positive number. 
Then
\begin{eqnarray*}
Z^{\text{test}} 
&=&\frac{\overline{X_{n}} -(m_0+\epsilon -\epsilon) }{\sqrt{\sigma _0 ^2/n}}\\
&=& Z^{\text{Truth}} + \frac{\epsilon}{\sqrt{\sigma _0 ^2/n}}\\
\end{eqnarray*}
where $Z^{\text{Truth} }:=(\overline{X_{n}} -(m_0+\epsilon ))/\sqrt{\sigma _0 ^2/n}$. 


In the following, we calculate the probability with which we reject the null hypothesis $H_0$ with confidence level $\alpha$.
\begin{eqnarray*}
\text{Prob}(Z^{\text{test}} >z_{2\alpha} )
&=&\text{Prob} (Z^{\text{Truth} } + \frac{\epsilon}{\sqrt{\sigma _0 ^2/n}} >z_{2\alpha})\\
&=&\text{Prob} (Z^{\text{Truth} } >z_{2\alpha} - \frac{\epsilon}{\sqrt{\sigma _0 ^2/n}})\\
&=&\text{Prob} (Z^{\text{Truth} } >z_{2\alpha} - \frac{\epsilon}{\sigma _0 }\sqrt{n} )\\
\end{eqnarray*}
Note that $\epsilon /\sigma _0$ is called the effect size.

Thus, if $z_{2\alpha} - \epsilon \sqrt{n} /\sigma _0 < z_{2(1-\beta)}$, i.e., if $n > ( z_{2\alpha}- z_{2(1-\beta)})^2 \sigma _0 ^2 \epsilon ^{-2}$, then the probability that the null hypothesis is rejected is greater than $1 - \beta$. 



For example, consider the case $\sigma _0 =1$, $\alpha =0.05$, and $(1-\beta) =\alpha$, then $z_{2\alpha}=1.28$ and in this case, for all $\epsilon>0$, if $n > 7 \epsilon ^{-2}$ then the probability in which above hypothesis test concludes that the difference of the observed mean from the hypothesized mean is significant is greater than $0.95$. This means that almost always the p-value is less than 0.05. Thus a large sample size induces a small p-value. 

For example,

 - if $\epsilon =1$ then by taking a sample size such that $n > 7$, then almost always the conclusion of the test will be that the observed difference is statistically significant.
Similarly,

 - if $\epsilon =0.1$ then by taking a sample size such that $n > 700$, then almost all tests will reach the conclusion that the difference is significant; and
 
 - if $\epsilon =0.01$ then by taking sample size so that $n > 70000$, then the same problem will arise.
 
This phenomenon also means that in large samples statistical tests will detect very small differences between populations.

By above consideration we can get the result ``significance difference'' with respect to any tiny difference $\epsilon$ by collecting a large enough sample $n$, and thus we must not use the statistical test. 


##### Acknowledge

 - Some famous cognitive psychologist read my another paper and his reply and suggestion are very kind, honesty, and politeness, ..., so, here I want to say thank you, I appreciate him.

 - In stack over flows unknown people and stan developers tells me technical programming materials, I also appreciate them.
