% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/validation_error_srsc.R
\name{validation.dataset_srsc}
\alias{validation.dataset_srsc}
\title{Errors of Estimator for any Given true parameter}
\usage{
validation.dataset_srsc(
  replicate.datset = 3,
  ModifiedPoisson = FALSE,
  mean.truth = 0.6,
  sd.truth = 5.3,
  z.truth = c(-0.8, 0.7, 2.38),
  NL = 259,
  NI = 57,
  ite = 1111,
  cha = 1,
  summary = TRUE,
  see = 1234,
  verbose = FALSE,
  serial.number = 1,
  base_size = 0,
  absolute.errors = TRUE
)
}
\arguments{
\item{replicate.datset}{A Number indicate
that how many you replicate dataset
from user's specified dataset.}

\item{ModifiedPoisson}{Logical, that is \code{TRUE} or \code{FALSE}.

If \code{ModifiedPoisson = TRUE},
then Poisson rate of false alarm is calculated \strong{\emph{per lesion}},
and a model is fitted
so that the FROC curve is an expected curve
 of points consisting of the pairs of TPF per lesion and FPF  \strong{\emph{per lesion}}.

Similarly,

If \code{ModifiedPoisson = TRUE},
then Poisson rate of false alarm is calculated \strong{\emph{per image}},
and a model is fitted
so that the FROC curve is an expected curve
 of points consisting of the pair of TPF per lesion and FPF  \strong{\emph{per image}}.



For more details, see the author's paper in which I explained \emph{per image} and \emph{per lesion}.
(for details of models, see   \href{https://cran.r-project.org/package=BayesianFROC}{ vignettes  }, now, it is omiited from this package, because the size of vignettes are large.)

If \code{ModifiedPoisson = TRUE},
 then the \emph{False Positive Fraction (FPF)} is defined as follows
 (\eqn{F_c} denotes the number of false alarms with confidence level \eqn{c} )


\deqn{ \frac{F_1+F_2+F_3+F_4+F_5}{N_L}, }

\deqn{ \frac{F_2+F_3+F_4+F_5}{N_L}, }

 \deqn{ \frac{F_3+F_4+F_5}{N_L}, }

  \deqn{ \frac{F_4+F_5}{N_L}, }

   \deqn{ \frac{F_5}{N_L}, }

where \eqn{N_L} is a number of lesions (signal).
To emphasize its denominator  \eqn{N_L},
we also call it the \emph{False Positive Fraction (FPF)} \strong{per lesion}.


On the other hand,


if \code{ModifiedPoisson = FALSE} (Default), then
\emph{False Positive Fraction (FPF)} is given by

\deqn{ \frac{F_1+F_2+F_3+F_4+F_5}{N_I}, }

\deqn{ \frac{F_2+F_3+F_4+F_5}{N_I}, }

 \deqn{ \frac{F_3+F_4+F_5}{N_I}, }

  \deqn{ \frac{F_4+F_5}{N_I}, }

   \deqn{ \frac{F_5}{N_I}, }

where \eqn{N_I} is the number of images (trial).
To emphasize its denominator \eqn{N_I},
we also call it the \emph{False Positive Fraction (FPF)} \strong{per image}.


The model is fitted so that
the estimated FROC curve can be ragraded
 as the expected pairs of   FPF per image and TPF per lesion (\code{ModifiedPoisson = FALSE })

 or as the expected pairs of   FPF per image and TPF per lesion  (\code{ModifiedPoisson = TRUE})

If \code{ModifiedPoisson = TRUE}, then FROC curve means the expected pair of FPF \strong{per lesion} and TPF.

On the other hand, if  \code{ModifiedPoisson = FALSE}, then FROC curve means the expected pair of \strong{FPF per image} and TPF.




So,data of FPF and TPF are changed thus, a fitted model is also changed whether  \code{ModifiedPoisson = TRUE} or \code{FALSE}.
In traditional FROC analysis, it uses only per images (trial). Since we can divide one image into two images or more images, number of
trial is not important. And more important is per signal. So, the author also developed FROC theory to consider FROC analysis under per signal.
One can see that the FROC curve is rigid with respect to change of a number of images, so, it does not matter whether \code{ModifiedPoisson = TRUE} or \code{FALSE}.
This rigidity of curves means that the number of images is redundant parameter for the FROC trial and
thus the author try to exclude it.


Revised 2019 Dec 8
Revised 2019 Nov 25
Revised 2019 August 28}

\item{mean.truth}{This is a parameter
of the latent Gaussian assumption
for the noise distribution.}

\item{sd.truth}{This is a parameter of the latent
Gaussian assumption for the noise distribution.}

\item{z.truth}{This is a parameter of the
latent Gaussian assumption for the noise distribution.}

\item{NL}{Number of Lesions.}

\item{NI}{Number of Images.}

\item{ite}{A variable to be passed to the function \code{rstan::}\code{sampling}() of \pkg{rstan}  in which it is named \code{iter}. A positive integer representing  the  number of samples synthesized by Hamiltonian Monte Carlo method,
and, Default = 1111}

\item{cha}{A variable to be passed to the function \code{rstan::}\code{sampling}() of \pkg{rstan}  in which it is named \code{chains}.  A positive integer representing   the number of chains generated by Hamiltonian Monte Carlo method,
and, Default = 1.}

\item{summary}{Logical: \code{TRUE} of \code{FALSE}. Whether to print the verbose summary. If \code{TRUE} then verbose summary is printed in the \R console. If \code{FALSE}, the output is minimal. I regret, this variable name should be verbose.}

\item{see}{A variable to be passed to the function \code{rstan::}\code{sampling}() of \pkg{rstan}  in which it is named \code{seed}.  A positive integer representing  seed used in stan,
Default = 1234.}

\item{verbose}{A logical, if \code{TRUE}, then the redundant summary is printed in \R console.
If \code{FALSE}, it suppresses output from this function.}

\item{serial.number}{A positive integer
or Character. This is for programming perspective.
The author use this to print the serial
 numbre of validation. This will be used
 in the validation function.}

\item{base_size}{An numeric for size of object, this is for the package developer.}

\item{absolute.errors}{A logical specifying whether  mean of errors is defined by

\describe{
\item{     \code{TRUE}                           }{ \eqn{ \bar{\epsilon}(\theta_0,N_I,N_L)}=  \eqn{ \frac{1}{K} \sum     | \epsilon_k | }   }
\item{     \code{FALSE}                          }{ \eqn{ \bar{\epsilon}(\theta_0,N_I,N_L)}=  \eqn{ \frac{1}{K} \sum       \epsilon_k   }   }
 }}
}
\value{
Return values is,

\describe{
\item{ Stanfit objects           }{  for each Replicated datasets   }
\item{ Errors                    }{ EAPs minus true values, in the above notations, it is \eqn{ \bar{\epsilon}(\theta_0,N_I,N_L)}    }
\item{  Variances of estimators. }{ This calculates the vaiance of posterior means over all replicated datasets   }
}
}
\description{
This function replicates many datasets from a model with a given model parameter as truth.
 Then fit a model to these datasets and get estimates.
 Then calculates mean or variance of the difference of estimates and truth.


This function gives a validation under the assumption that
we obtain a fixed true model parameter drawn from the prior.

But, you know, the author noticed that it is not sufficient  because
, in Bayesian, such a true parameter is merely one time sample from
prior. So, what we should do is to draw randomly many samples from priors.

Well, I know, but, I am being such a couch potato. In the future, I would do,
if I was alive.
Even if I try this, this effort never take account by someone.
But, to live, it is required. No money, no life. 2020 NOv 29



In the future, what I should do is that the following calculations.


1. Draw a sample of parameter  \eqn{\theta} from prior, namely,  \eqn{\theta \sim \pi(\theta)}

2. Draw a sample of data \eqn{x=x(\theta)} from  a model with the sample of prior \eqn{ x \sim \pi(x|\theta)}

3. Draw a sample of parameter \eqn{\theta'=\theta'(x(_\theta))} from  a posterior with the sample of data \eqn{ \theta' \sim \pi(\theta|x)}

4. Calculates the integral \eqn{ \int | \theta' - \theta|^2 \pi(\theta)d\theta} as the error of estimates among all priors..
}
\details{
Let us denote a  model parameter by \eqn{\theta_0}
\eqn{N_I} by a number of
images  and number
 of lesions  by \eqn{N_L} which are specified
  by user as the variables of the function.

\describe{
\item{ \strong{(I)} Replicates models for datasets \eqn{D_1,D_2,...,D_k,...,D_K}.        }{          }
\item{ Draw a dataset \eqn{D_k}  from a likelihood (model), namely              }{ \eqn{D_k \sim likelihood(|\theta_0)}.                                                                  }
\item{ Draw a MCMC samples  \eqn{\{ \theta_i (D_k)\}} from a posterior, namely  }{ \eqn{ \theta _i \sim \pi(|D_k)}.                                                                       }
\item{ Calculate  a posterior mean,  namely                                     }{ \eqn{ \bar{\theta}(D_k) := \sum_i \theta_i(D_k) }.                                                  }
\item{ Calculates error for  \eqn{D_k}                                          }{ \eqn{\epsilon_k}:=Trueth - posterior mean estimates of  \eqn{D_k} =  \eqn{|\theta_0   - \bar{\theta}(D_k)|} (or  =  \eqn{\theta_0   - \bar{\theta}(D_k)}, accordinly by the user specified \code{absolute.errors} ).                       }
\item{ \strong{(II)} Calculates mean of errors                                  }{ mean of errors  \eqn{ \bar{\epsilon}(\theta_0,N_I,N_L)}=  \eqn{ \frac{1}{K} \sum     \epsilon_k }   }
}

 Running this function, we can see that the error  \eqn{ \bar{\epsilon}(\theta_0,N_I,N_L)} decreases
 monotonically as a given number of images \eqn{N_I} or a given number of lesions \eqn{N_L} increases.

 Also, the scale of error also will be found. Thus this function can show how our estimates are correct.
Scale of error differs for each componenet of model parameters.



Revised 2019  August 28
}
\examples{
\dontrun{
#===========================    The first example  ======================================


#   It is sufficient to run the function with default variable

   datasets <- validation.dataset_srsc()


# Today 2020 Nov 29 I have completely forgotten this function, oh it helps me. Thank me.
#=============================  The second example ======================================

#   If user does not familiar with the values of thresholds, then
#   it would be better to use the actual estimated values
#    as an example of true parameters. In the following,
#     I explain this.
# First, to get posterior mean estimates, we run the following:



  fit <- fit_Bayesian_FROC(dataList.Chakra.1,ite = 1111,summary =FALSE,cha=3)






#  Secondly, extract the expected a posterior estimates (EAPs) from the object fit
# Note that EAP is also called "posterior mean"

  z <- rstan::get_posterior_mean(fit,par=c("z"))[,"mean-all chains"]





#  Thirdly we use this z as a true value.


   datasets <- validation.dataset_srsc(z.truth = z)



#========================================================================================
#            1)             extract replicated fitted model object
#========================================================================================


    # Replicates models

    a <- validation.dataset_srsc(replicate.datset = 3,ite = 111)



    # Check convergence, in the above MCMC iterations = 111 which is too small to get
    # a convergence MCMC chain, and thus the following example will the example
    # of a non-convergent model in the r hat criteria.

    ConfirmConvergence( a$fit[[3]])


    # Check trace plot to confirm whether MCMC chain converge or not.

    stan_trace( a$fit[[3]],pars = "A")


   # Check p value, for chi square goodness of fit whose null hypothesis is that
   # the model is fitted well.

   fit@posterior_predictive_pvalue_for_chi_square_goodness_of_fit












                                          # Revised in 2019 August 29

                                          # Revised in 2020 Nov 28
                                          # It is weird, awesome,
                                          # What a fucking English,...I fix it.




#========================================================================================
#            1)            Histogram of error of postrior means for replicated datasets
#========================================================================================
#'

  a<-   validation.dataset_srsc(replicate.datset = 100)
  hist(a$error.of.AUC,breaks = 111)
  hist(a$error.of.AUC,breaks = 30)






#========================================================================================
#                             absolute.errors = FALSE generates negative biases
#========================================================================================


 validation.dataset_srsc(absolute.errors = FALSE)



#========================================================================================
#      absolute.errors = TRUE coerce negative biases to positives, i.e., L^2 norm
#========================================================================================


 validation.dataset_srsc(absolute.errors = TRUE)








#========================================================================================
#     Check each  fitted model object
#========================================================================================




a <- validation.dataset_srsc(verbose = TRUE)

a$fit[[2]]



class(a$fit[[2]])

rstan::traceplot(a$fit[[2]], pars = c("A"))






#========================================================================================
#     NaN ... why? 2021 Dec
#========================================================================================

fits <- validation.dataset_srsc()

f <-fits$fit[[1]]
rstan::extract(f)$dl
sum(rstan::extract(f)$dl)
Is.nan.in.MCMCsamples <- as.logical(!prod(!is.nan(rstan::extract(f)$dl)))
rstan::extract(f)$A[525]
a<-rstan::extract(f)$a[525]
b<-rstan::extract(f)$b[525]

Phi(  a/sqrt(b^2+1)  )
x<-rstan::extract(f)$dl[2]

a<-rstan::extract(f)$a
b<-rstan::extract(f)$b

a/(b^2+1)
Phi(a/(b^2+1))
mean( Phi(a/(b^2+1))  )

#'




}# dontrun
}
