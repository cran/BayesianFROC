% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ppp.R
\name{ppp_srsc}
\alias{ppp_srsc}
\title{Calculates PPP for Models of a single reader and a single modality (Calculation is correct! :'-D)}
\usage{
ppp_srsc(
  StanS4class,
  Colour = TRUE,
  dark_theme = TRUE,
  plot = TRUE,
  summary = FALSE,
  plot_data = TRUE,
  replicate.number.from.model.for.each.MCMC.sample = 100
)
}
\arguments{
\item{StanS4class}{An S4 object of class \emph{\code{ \link{stanfitExtended}}} which is an inherited class from the S4 class  \code{stanfit}.
This \R object is a fitted model object
 as a return value of the function \code{\link{fit_Bayesian_FROC}()}.

To be passed to \code{\link{DrawCurves}()}, \code{\link{ppp}()}  and ... etc}

\item{Colour}{Logical: \code{TRUE} of \code{FALSE}.  whether Colour of curves is dark theme or not.}

\item{dark_theme}{TRUE or FALSE}

\item{plot}{Logical, whether replicated data are drawn, in the following notation, replicated data are denoted by \eqn{y_1,y_2,...,y_N}.}

\item{summary}{Logical: \code{TRUE} of \code{FALSE}. Whether to print the verbose summary. If \code{TRUE} then verbose summary is printed in the \R console. If \code{FALSE}, the output is minimal. I regret, this variable name should be verbose.}

\item{plot_data}{A logical, whether data is plotted in the plot of data synthesized from the posterior predictive distribution
I cannot understand what I wrote in the past. My head is crazy cuz I was MCS, head inflammation maybe let me down.

Suppose that  \deqn{\theta_1, \theta_2, \theta_3,...,\theta_N} are samples drawn in \eqn{N} times from posterior \eqn{\pi(\theta|D)} of given data \eqn{D}.
So, these \eqn{\theta_i;i=1,2,...} are contained in a stanfit object specified as the variable \code{StanS4class}.


Let \eqn{y_1,y_2,...,y_n} be samples drawn as the manner

\deqn{y_1 \sim likelihood ( . |\theta_1), }
\deqn{y_2 \sim likelihood ( . |\theta_2),}
\deqn{y_3 \sim likelihood ( .|\theta_3),}
\deqn{...,}
\deqn{y_N \sim likelihood ( . |\theta_N).}

We repeat this in \eqn{J} times, namely,
we draw the samples \eqn{y_{n,j},n=1,..,N;j=1,...,J} so that



\deqn{y_{1,j} \sim likelihood ( . |\theta_1), }
\deqn{y_{2,j} \sim likelihood ( . |\theta_2),}
\deqn{y_{3,j} \sim likelihood ( .|\theta_3),}
\deqn{...,}
\deqn{y_{n,j} \sim likelihood ( . |\theta_n),}
\deqn{...,}
\deqn{y_{N,j} \sim likelihood ( . |\theta_N).}


Yes, the variable \code{replicate.number.from.model.for.each.MCMC.sample} means \eqn{J}!
We can write it more explicitly without abbreviation as follows.


\deqn{y_{1,1},y_{1,2},...,y_{1,j},...,y_{1,J} \sim likelihood ( . |\theta_1), }
\deqn{y_{2,1},y_{2,2},...,y_{2,j},...,y_{2,J} \sim likelihood ( . |\theta_2), }
\deqn{y_{3,1},y_{3,2},...,y_{3,j},...,y_{3,J} \sim likelihood ( . |\theta_3), }
\deqn{...,}
\deqn{y_{n,1},y_{n,2},...,y_{n,j},...,y_{n,J} \sim likelihood ( . |\theta_n), }
\deqn{...,}
\deqn{y_{N,1},y_{N,2},...,y_{N,j},...,y_{N,J} \sim likelihood ( . |\theta_N). }




Now, my body is not so good, so, I am tired. Cuz I counld not understand what I wrote, so I reviesed in 2020 Aug 9.

You health is very bad condition, so, if the sentence is not clear, it is also for me! even if I wrote it! So, If I notice that my past brain is broken, then I will revise. Ha,,, I want be rest in peace.}

\item{replicate.number.from.model.for.each.MCMC.sample}{A positive integer, representing \eqn{J} in the following notation.}
}
\value{
A list, including p value and materials to calculate it.

Contents of the list as a return values is the following:
\describe{
\item{ \code{FPF,TPF,..etc}      }{ data \eqn{y_{n,j} \sim likelihood ( . |\theta_n),}    }
\item{ \code{chisq_at_observed_data}      }{ \deqn{\chi (D|\theta_1), \chi (D|\theta_2), \chi (D|\theta_3),...,\chi (D|\theta_n),} }
\item{ \code{chisq_not_at_observed_data}  }{\deqn{\chi (y_1|\theta_1), \chi (y_2|\theta_2), \chi (y_3|\theta_3),...,\chi (y_n|\theta_n),  }}
\item{ \code{Logical}                     }{ The i-th component is a logical vector indicating whether  \deqn{\chi (y_2|\theta_2) > \chi (D|\theta_2)} is satisfied or not. Oppai ga Ippai. If \code{TRUE}, then the inequality holds.}
\item{ \code{p.value}                     }{  From the component \code{Logical}, we calculate the so-called \emph{Posterior Predictive P value}. Note that the author hate this notion!! I hate it!! Akkan Beeeee!!! }
}
}
\description{
Calculates Posterior Predictive P value for chi square (goodness of fit)




\strong{Appendix: p value}




In order to evaluate the goodness of fit of our model to the data, we used the so-called the posterior predictive p value.

In the following, we use general conventional notations.
Let \eqn{y_{obs} } be an observed dataset and \eqn{f(y|\theta)} be a model (likelihood) for future dataset \eqn{y}. We denote a prior and a posterior distribution by \eqn{\pi(\theta)} and \eqn{\pi(\theta|y) \propto f(y|\theta)\pi(\theta)}, respectively.

In our case, the data \eqn{y} is a pair of hits and false alarms; that is, \eqn{y=(H_1,H_2, \dots H_C; F_1,F_2, \dots F_C)} and \eqn{\theta = (z_1,dz_1,dz_2,\dots,dz_{C-1},\mu, \sigma)  }. We define the \eqn{\chi^2} discrepancy (goodness of fit statistics) to validate that our model fit the data.
\deqn{ T(y,\theta) := \sum_{c=1,.......,C} \biggr( \frac{\bigl(H_c-N_L\times p_c(\theta) \bigr)^2}{N_L\times p_c(\theta)}+  \frac{\bigl(F_c- q_{c}(\theta) \times N_{X}\bigr)^2}{ q_{c}(\theta) \times N_{X} }\biggr). }



for a single reader and a single modality.


\deqn{   T(y,\theta) := \sum_{r=1}^R \sum_{m=1}^M \sum_{c=1}^C \biggr( \frac{(H_{c,m,r}-N_L\times p_{c,m,r}(\theta))^2}{N_L\times p_{c,m,r}(\theta)}+ \frac{\bigl(F_c- q_{c}(\theta) \times N_{X}\bigr)^2}{ q_{c}(\theta) \times N_{X} }\biggr).}

for multiple readers and multiple modalities.




Note that \eqn{p_c} and \eqn{\lambda _{c}} depend on \eqn{\theta}.





In classical frequentist methods, the parameter \eqn{\theta} is a fixed estimate, e.g., the maximal likelihood estimator. However, in a Bayesian context, the parameter is not deterministic. In the following, we show the p value in the Bayesian sense.


Let \eqn{y_{obs}} be an observed dataset (in an FROC context, it is hits and false alarms). Then, the so-called \emph{posterior predictive p value} is defined by

\deqn{     p_value   = \int \int  \, dy\, d\theta\, I(  T(y,\theta) > T(y_{obs},\theta) )f(y|\theta)\pi(\theta|y_{obs})  }



In order to calculate the above integral, let  \eqn{\theta_1,\theta _2, ......., \theta_i,.......,\theta_I} be samples from the posterior distribution of \eqn{ y_{obs} }, namely,

\deqn{  \theta_1  \sim \pi(....|y_{obs} ),}
\deqn{ .......,}
\deqn{ \theta_i  \sim \pi(....|y_{obs} ),}
\deqn{ .......,}
\deqn{  \theta_I \sim \pi(....|y_{obs}  ).}


we obtain a sequence of models (likelihoods), i.e.,  \eqn{f(....|\theta_1),f(....|\theta_2),......., f(....|\theta_n)}.
We then draw the samples \eqn{y^1_1,....,y^i_j,.......,y^I_J }, such that each \eqn{y^i_j} is a sample from the distribution whose density function is \eqn{f(....|\theta_i)}, namely,

\deqn{ y^1_1,.......,y^1_j,.......,y^1_J  \sim f(....|\theta_1),}
\deqn{.......,}
\deqn{ y^i_1,.......,y^i_j,.......,y^i_J  \sim f(....|\theta_i),}
\deqn{.......,}
\deqn{ y^I_1,.......,y^I_j,.......,y^I_J   \sim f(....|\theta_I).}


Using the Monte Carlo integral twice, we calculate the  integral of any function \eqn{ \phi(y,\theta)}.

\deqn{ \int \int  \, dy\, d\theta\, \phi(y,\theta)f(y|\theta)\pi(\theta|y_{obs}) }
\deqn{\approx  \int \,  \frac{1}{I}\sum_{i=1}^I \phi(y,\theta_i)f(y|\theta_i)\,dy}
\deqn{    \frac{1}{IJ}\sum_{i=1}^I \sum_{j=1}^J \phi(y^i_j,\theta_i)}









In particular, substituting \eqn{\phi(y,\theta):= I(  T(y,\theta) > T(y_{obs},\theta) ) } into the above equation,
we can approximate  the posterior predictive p value.


\deqn{    p_value   \approx  \frac{1}{IJ}\sum_i \sum_j  I(  T(y^i_j,\theta_i) > T(y_{obs},\theta_i) ) }
}
\details{
In addition, this function plots replicated datasets from model at each MCMC sample generated by HMC.
Using the Hamiltonian Monte Carlo Sampling: HMC.
we can draw the MCMC
samples of size  \eqn{n}, say  \deqn{\theta_1, \theta_2, \theta_3,...,\theta_n },
namely,
\deqn{\theta_1  \sim \pi(.|D), }
\deqn{\theta_2 \sim \pi(.|D), }
\deqn{\theta_3  \sim \pi(.|D),}
\deqn{...,}
\deqn{\theta_n  \sim \pi(.|D).}
   where \eqn{\pi(\theta|D)} is the posterior for given data \eqn{D}.

We draw samples as follows.


\deqn{y_{1,1},y_{1,2},...,y_{1,j},...,y_{1,J} \sim likelihood ( . |\theta_1), }
\deqn{y_{2,1},y_{2,2},...,y_{2,j},...,y_{2,J} \sim likelihood ( . |\theta_2), }
\deqn{y_{3,1},y_{3,2},...,y_{3,j},...,y_{3,J} \sim likelihood ( . |\theta_3), }
\deqn{...,}
\deqn{y_{n,1},y_{n,2},...,y_{n,j},...,y_{n,J} \sim likelihood ( . |\theta_n), }
\deqn{...,}
\deqn{y_{N,1},y_{N,2},...,y_{N,j},...,y_{N,J} \sim likelihood ( . |\theta_N).}

Then we calculates the chi-squares for each sample.

\deqn{ \chi(y_{1,1}|\theta_1), \chi(y_{1,2}|\theta_1), \chi(y_{1,3}|\theta_1),..., \chi(y_{1,j}|\theta_1),...., \chi(y_{1,J}|\theta_1),}
\deqn{ \chi(y_{2,1}|\theta_2), \chi(y_{2,2}|\theta_2), \chi(y_{2,3}|\theta_2),..., \chi(y_{2,j}|\theta_2),...., \chi(y_{2,J}|\theta_2),}
\deqn{ \chi(y_{3,1}|\theta_3), \chi(y_{3,2}|\theta_3), \chi(y_{3,3}|\theta_3),..., \chi(y_{3,j}|\theta_3),...., \chi(y_{3,J}|\theta_3),}
\deqn{...,}
\deqn{ \chi(y_{i,1}|\theta_i), \chi(y_{i,2}|\theta_i), \chi(y_{i,3}|\theta_i),..., \chi(y_{i,j}|\theta_i),...., \chi(y_{I,J}|\theta_i),}
\deqn{...,}
\deqn{ \chi(y_{I,1}|\theta_I), \chi(y_{I,2}|\theta_I), \chi(y_{I,3}|\theta_I),..., \chi(y_{I,j}|\theta_I),...., \chi(y_{I,J}|\theta_I).}


where \eqn{L ( . |\theta_i)} is a likelihood at parameter \eqn{\theta_i}.



Let \eqn{ \chi(y|\theta)  } be a chi square goodness of fit statistics of our hierarchical Bayesian Model


\deqn{\chi(y|\theta) := \sum_{r=1}^R \sum_{m=1}^M \sum_{c=1}^C ( \frac { ( H_{c,m,r}-N_L\times p_{c,m,r})^2}{N_L\times p_{c,m,r}} + \frac{(F_{c,m,r}-(\lambda_{c} -\lambda_{c+1} )\times N_{L})^2}{(\lambda_{c} -\lambda_{c+1} )\times N_{L} }).}


and a chi square goodness of fit statistics of our non-hierarchical Bayesian Model

\deqn{\chi(y|\theta) :=  \sum_{c=1}^C \biggr( \frac{( H_{c}-N_L\times p_{c})^2}{N_L\times p_{c}} + \frac{(F_{c}-(\lambda_{c} -\lambda_{c+1} ) )\times N_{L}]^2}{(\lambda_{c} -\lambda_{c+1} )\times N_{L} }\biggr).}

where  a dataset \eqn{y} denotes \eqn{ (F_{c,m,r}, H_{c,m,r}) } in MRMC case and  \eqn{ (F_{c}, H_{c}) } in a single reader and a single modality case,
and model parameter \eqn{\theta}.


Then we can calculate the \emph{posterior predictive p value} for a given dataset \eqn{y_0}.

\deqn{ \int \int I( \chi(y|\theta) > \chi(y_0|\theta) )   f(y|\theta) \pi(\theta|y_0) d \theta d y  }
\deqn{ \approx \int \sum_i I( \chi(y|\theta_i) > \chi(y_0|\theta_i) )   f(y|\theta_i) d y   }
\deqn{ \approx \sum_{j=1}^J \sum_{i=1}^I I( \chi(y_{i,j}|\theta_i) > \chi(y_0|\theta_i) )     }



  When we plot these synthesized data-sets \eqn{y_{i,j}}, we use the \code{jitter()} which adds a small amount of noise to \strong{avoid overlapping points}.
For example, \code{jitter(c(1,1,1,1))} returns  values: \code{1.0161940 1.0175678 0.9862400 0.9986126}, which is changed
from \code{1,1,1,1} to be not exactly 1 by adding tiny errors to avoid overlapping. I love you. 2019 August 19
Nowadays, I cannot remove my self from some notion, such as honesty, or pain, or,.. maybe these thing is no longer with myself.
This programm is made to fix previous release calculation. Now, this programm calculates correct p value.

So... I calculate the ppp for MCMC and Graphical User Interface based on Shiny for MRMC, which should be variable such as
number of readers, modalities, to generate such ID vectors automatically. Ha,... tired! Boaring, I want to die...t, diet!!
Tinko, tinko unko unko. Manko manko. ha.

Leberiya, he will be die, ha... he cannot overcome, very old, old guy.
 I will get back to meet him. Or I cannot meet him? Liberiya,...very wisdom guy,
Ary you already die? I will get back with presents for you. Ball, I have to throgh ball, and he will catch it.


The reason why the author made the plot of data drawn from \strong{Posterior Predictive likelihoods with each MCMC parameters} is
to understand our programm is correct, that is, each drawing is very mixed. Ha,.... when wright this,... I always think who read it.
I love you, Ruikobach. Ruikobach is tiny and tiny, but,... cute. Ruikosan...Ruiko...
But he has time only several years. He will die, he lives sufficiently so long, ha.

Using this function, user would get \strong{reliable posterior predictive p values}, Cheers! Pretty Crowd!



We note that the calculation of posterior perdictive p value (PPP) relies on the law of large number.
Thus, in order to obtain the relicable PPP, we need to enough large MCMC samples to approximate
the double integral of PPP.
For example, the MCMC samples is small, then R hat is far from 1 but, the low MCMC samples leads
us to incorrect p value which sometimes said that the model is correct even if the R hat criteria
reject the MCMC results.
}
\examples{


\dontrun{

#========================================================================================
#            1) Create a fitted model object with data named  "d"
#========================================================================================



fit <- fit_Bayesian_FROC( dataList = d,
                              ite  = 222 # to restrict running time, but it is too small
                           )



#========================================================================================
#            2) Calculate p value and meta data
#========================================================================================



            ppp <- ppp_srsc(fit)



#========================================================================================
#            3) Extract a p value
#========================================================================================




              ppp$p.value


# Revised 2019 August 19
# Revised 2019 Nov 27

     Close_all_graphic_devices() # 2020 August
}



}
\author{
Issei Tsunoda, Prof. of Curlbus University, Mosquitobus and Gostbus univ. also. My technique of catch mosquitos are execellent, so, I am a prof. ha,, employ me. My health is bad, my life will be over.
}
