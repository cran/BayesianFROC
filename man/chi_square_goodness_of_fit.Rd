% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/chi_square_goodness_of_fit.R
\name{chi_square_goodness_of_fit}
\alias{chi_square_goodness_of_fit}
\title{A vector of the Goodness of Fit (Chi Square)
for each MCMC samples}
\usage{
chi_square_goodness_of_fit(StanS4class, dig = 3,
  h = StanS4class@dataList$h, f = StanS4class@dataList$f)
}
\arguments{
\item{StanS4class}{An S4 object of class \emph{\code{ \link{stanfitExtended}}} which is an inherited class from the S4 class  \code{\link[rstan]{stanfit}}  that can be passed to the \code{\link{DrawCurves}()}, \code{\link{ppp}()}  and ... etc}

\item{dig}{To be passed to the function \code{rstan::}\code{\link[rstan]{sampling}}() in \pkg{rstan}. An argument of \code{rstan::}\code{\link[rstan]{sampling}}()  in which it is named \code{...??}.   A positive integer representing   the Significant digits, used in stan Cancellation.
default = 5,}

\item{h}{A vector of positive integers,
representing the number of hits.
This variable was made in order to
 substitute the hits data drawn
 from the posterior predictive distributions.
In famous Gelman's book, he explain how
to use the test statistics in the Bayesian context.
In this context I need to substitute the replication
 data from the posterior predictive distributions.}

\item{f}{A vector of positive integers,
representing the number of false alarms.
This variable was made in order to
 substitute the false alarms data drawn
 from the posterior predictive distributions.
In famous Gelman's book, he explain
how to use the test statistics in the
Bayesian context. In this context I need
to substitute the replication data from
the posterior predictive distributions.}
}
\value{
Chi squares for each MCMC samples.
So, the return values is a vector
whose length is number of MCMC iterations
 except the warming up period.
 Of course if MCMC is not only one chain,
  then such samples are used over the all chains.

  In the sequal, we use the notation
  for prior \eqn{\pi(\theta)}, posterior
  \eqn{\pi(\theta|D)}, lilelihood
   \eqn{f(D|\theta)}, parameter \eqn{\theta},
   datasets \eqn{D} as follows;

 \deqn{ \pi(\theta|D) \propto f(D|\theta) \pi(\theta).}



 Let us denote the \strong{posterior MCMC samples} of
 size  \eqn{N} by

   \deqn{\theta_1, \theta_2, \theta_3,...,\theta_N}


   which is drawn from
   posterior \eqn{\pi(\theta|D)} of given data \eqn{D}.


Recall that the chi square goodness of fit statistics \eqn{\chi}
depends on the model parameter \eqn{\theta} and data \eqn{D}, namely,

\deqn{\chi^2 = \chi^2 (D|\theta)}.

Then return value is a vector
of length \eqn{N} whose components is given by:



\deqn{\chi^2 (D|\theta_1), \chi^2 (D|\theta_2), \chi^2 (D|\theta_3),...,\chi^2 (D|\theta_N),}

which is a vector and  a return value of this function.

 As an application of this return value,
  we  can calculate
   the posterior mean of \eqn{\chi = \chi (D|\theta)},
    namely, we get

\deqn{ \chi^2 (D) =\int \chi^2 (D|\theta)  \pi(\theta|D)     d\theta.}



In my model, almost all example,
 result of calculation shows that


\deqn{  \int \chi^2 (D|\theta)  \pi(\theta|D)     d\theta > \chi^2 (D| \int \theta   \pi(\theta|D)     d\theta) }


The above inequality is true for all \eqn{D}?? I conjecture it.

Revised 2019 August 18
Revised 2019 Sept. 1


 Our data is \strong{2C categories}, that is,


 the number of hits        :h[1], h[2], h[3],...,h[C] and

 the number of false alarms: f[1],f[2], f[3],...,f[C].


   Our model has \strong{C+2 parameters}, that is,

   the thresholds of the bi normal assumption z[1],z[2],z[3],...,z[C] and

   the mean and standard deviation of the signal distribution.


 So, the degree of freedom of this statistics is calculated by


     No. of categories  -  No. of parameters  - 1 = 2C-(C+2)-1 =C -3.

This differ from Chakraborty's  result C-2. Why ?
}
\description{
Chi square goodness of
fit statistics at each MCMC sample w.r.t. a dataset.
}
\details{
To calculate the chi square
test statistics, the two variables
are required; one is a observed dataset
 and the other is an estimated parameters.
In the classical chi square values,
MLE(maximal likelihood estimator) is used
for estimated parameters. In Bayesian sense,
the parameter is for each MCMC iterations,
that is, parameter is not deterministic and
we consider it is a random variable or
samples from the posterior distribution.
And such samples are obtained in the Hamiltonian
Monte Carlo Simulation.
Thus we can calculate chi square values for each MCMC samples.
}
\examples{

\donttest{

#  Get the MCMC samples from a dataset.

       fit <- fit_Bayesian_FROC(BayesianFROC::dataList.Chakra.1,
                           ite = 1111,
                           summary =FALSE,
                           cha = 2)

#   The chi square discrepancies are calculated by the following code

         Chi.Square.for.each.MCMC.samples   <-   chi_square_goodness_of_fit(fit)



#'


         # With Warning
         chi_square_goodness_of_fit(fit)

         # Without warning
          chi_square_goodness_of_fit(fit,
                                     h=fit@dataList$h,
                                     f=fit@dataList$f)







#  Get posterior mean of the chi square discrepancy.

                    m<-   mean(Chi.Square.for.each.MCMC.samples)




# The author read at 2019 Sept. 1, it helps him. Thanks me!!




# Calculate the p-value for the posterior mean of the chi square discrepancy.

                     stats::pchisq(m,df=1)


# Difference between chi sq. at EAP and EAP of chi sq.

   mean( fit@chisquare - chi_square_goodness_of_fit(fit))


}# dottest

}
